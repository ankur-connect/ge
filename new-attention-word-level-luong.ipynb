{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "You will need [PyTorch](http://pytorch.org/) to build and train the models, and [matplotlib](https://matplotlib.org/) for plotting training and visualizing attention outputs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from collections import Counter\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import math\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/carolineroper/Documents/School/Natural Language Processing/Char-NMT/data/train.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    DE_seq = []\n",
    "    EN_seq = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        DE_seq.append(line[0])\n",
    "        EN_seq.append(line[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing words\n",
    "\n",
    "We'll need a unique index per word to use as the inputs and targets of the networks later. To keep track of all this we will use a helper class called `Lang` which has word &rarr; index (`word2index`) and index &rarr; word (`index2word`) dictionaries, as well as a count of each word `word2count` to use to later replace rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and decoding files\n",
    "\n",
    "The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = s.replace(\" <EOS>\", \"\")\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) #separates punctuation from the word\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) #strips anything that isn't a character of punctuation\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(seq1, seq2):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    seq1 = [normalizeString(s) for s in seq1]\n",
    "    seq2 = [normalizeString(s) for s in seq2]\n",
    "    \n",
    "    pairs = list(map(list, zip(seq1, seq2)))\n",
    "\n",
    "    return pairs #, vocab1, vocab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    }
   ],
   "source": [
    "pairs = readLangs(DE_seq, EN_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering sentences\n",
    "\n",
    "Since there are a *lot* of example sentences and we want to train something quickly, we'll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes punctuation) and we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. (accounting for apostrophes being removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#will ultimately remove this and set MAX_LENGTH to either 50 like the paper or the longest sentence in corpus\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "pairs = filterPairs(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_vocabulary(seq, vocab_size):\n",
    "    counter = Counter()\n",
    "    for sentence in seq:\n",
    "        counter.update(sentence.split())\n",
    "    vocabulary = [count[0] for count in counter.most_common(vocab_size)]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_vocab = find_vocabulary([pair[0] for pair in pairs], 10000)\n",
    "output_vocab = find_vocabulary([pair[1] for pair in pairs], 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNK_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "PAD_token = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, vocab):\n",
    "        self.name = name\n",
    "        self.vocab = vocab\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = dict(zip(range(4,len(vocab)+4), vocab))\n",
    "        self.index2word[UNK_token] = \"<UNK>\"\n",
    "        self.index2word[SOS_token] = \"<SOS>\"\n",
    "        self.index2word[EOS_token] = \"<EOS>\"\n",
    "        self.index2word[PAD_token] = \"<PAD>\"\n",
    "        self.word2index = {v: k for k, v in self.index2word.items()}\n",
    "        self.n_words = len(self.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_lang = Lang('ger', input_vocab)\n",
    "output_lang = Lang('eng', output_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "* Read text file and split into lines, split lines into pairs\n",
    "* Normalize text, filter by length and content\n",
    "* Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence, seq_len):\n",
    "    #returns 0 if not found in word2index\n",
    "    indexes = [Counter(lang.word2index)[word] for word in sentence.split(' ')]\n",
    "    #adds EOS token at EOS\n",
    "    indexes.extend([EOS_token])\n",
    "    #trims to the seq len\n",
    "    indexes = indexes[0:(min(seq_len, len(indexes)))]\n",
    "    #pads if needed\n",
    "    indexes.extend([3] * (seq_len - len(indexes)))\n",
    "    return indexes\n",
    "\n",
    "def variableFromSentence(lang, sentence, seq_len):\n",
    "    indexes = indexesFromSentence(lang, sentence, seq_len)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if USE_CUDA:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "def variablesFromPair(pair, seq_len):\n",
    "    input_variable = variableFromSentence(input_lang, pair[0], seq_len)\n",
    "    target_variable = variableFromSentence(output_lang, pair[1], seq_len)\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "<img src=\"images/encoder-network.png\" style=\"float: right\" />\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, batch_size=BATCH_SIZE, dropout_p=0.2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size) #the input size is the number of words\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, dropout = dropout_p)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        seq_len = len(x)\n",
    "        embedded = self.embedding(x).view(seq_len, self.batch_size, -1)\n",
    "        output = self.dropout(embedded)\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        #the paper says to use \"uniform initialization of parameters in [âˆ’0.1,0.1]: does that include these?\"\n",
    "        h0 = Variable(torch.zeros(1, 1, self.hidden_size)) #I think maybe one of the 1's should be the batch_size?\n",
    "        c0 = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if USE_CUDA:\n",
    "            return h0.cuda(), c0.cuda()\n",
    "        else:\n",
    "            return h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size, method = 'dot', batch_size=BATCH_SIZE, max_length=MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        attn_energies = Variable(torch.zeros(self.batch_size, seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[:,i] = self.score(hidden, encoder_outputs[i])\n",
    "            \n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        # I confirmed that this computes row-wise, which is what we need\n",
    "        after_softmax = F.softmax(attn_energies)\n",
    "        return after_softmax.unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = torch.diag(torch.matmul(hidden, torch.transpose(encoder_output,0,1)))\n",
    "            #print (\"energy\", energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.dot(energy)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a decoder that plugs this Attn module in after the RNN to calculate attention weights, and apply those weights to the encoder outputs to get a context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, batch_size = BATCH_SIZE):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.attn = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, self.batch_size, -1) # S=1 x B x N\n",
    "        \n",
    "        # Combine embedded input word and last context, run through RNN\n",
    "        \n",
    "        #do I need to do this? the paper doesn't mention any type of rnn in the decoder\n",
    "        #rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        #rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(word_embedded.squeeze(0), encoder_outputs)\n",
    "\n",
    "        context = torch.bmm(attn_weights.transpose(0, 1), encoder_outputs.transpose(0, 1)) # B x 1 x N ####this throws an error now\n",
    "        \n",
    "        # Final output layer (next word prediction) using the hidden state and context vector\n",
    "        word_embedded = word_embedded.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((word_embedded, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        # If I were using the GRU, I'd output hidden, not last_hidden\n",
    "        return output, context, last_hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Defining a training iteration\n",
    "\n",
    "To train we first run the input sentence through the encoder word by word, and keep track of every output and the latest hidden state. Next the decoder is given the last hidden state of the decoder as its first hidden state, and the `<SOS>` token as its first input. From there we iterate to predict a next token from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clip = 5.0\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer,\\\n",
    "          criterion, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    # One SOS token for each stence, so length is batch size\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]*batch_size]))\n",
    "    decoder_context = Variable(torch.zeros(batch_size, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden[0] # Use last hidden state from encoder to start decoder\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "        \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "\n",
    "        # Get most likely word index (highest value) from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        \n",
    "        ni = topi.squeeze(1)\n",
    "\n",
    "        decoder_input = Variable(ni) # Chosen word is next input\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "        # Stop at end of sentence (not necessary when using known targets)\n",
    "        # can't do this if we batchify\n",
    "        # if ni == EOS_token: break\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally helper functions to print time elapsed and estimated time remaining, given the current time and progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running training\n",
    "\n",
    "With everything in place we can actually initialize a network and start training.\n",
    "\n",
    "To start, we initialize models, optimizers, and a loss function (criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "\n",
    "attn_model = Attn(hidden_size)\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers=4, batch_size = BATCH_SIZE)\n",
    "#(self, input_size, hidden_size, n_layers, batch_size=BATCH_SIZE, dropout_p=0.2):\n",
    "decoder = AttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, batch_size=BATCH_SIZE) \n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set up variables for plotting and tracking progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "n_epochs = 5\n",
    "plot_every = 5\n",
    "print_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually train, we call the train function many times, printing a summary as we go.\n",
    "\n",
    "*Note:* If you run this notebook you can train, interrupt the kernel, evaluate, and continue training later. You can comment out the lines above where the encoder and decoder are initialized (so they aren't reset) or simply run the notebook starting from the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH = MAX_LENGTH\n",
    "\n",
    "# Begin!\n",
    "variables = [variablesFromPair(pair, seq_len=SEQ_LENGTH) for pair in pairs]\n",
    "shuffle(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 53s (- 1m 53s) (5 50%) 7.4744\n",
      "2m 2s (- 0m 0s) (10 100%) 7.3330\n",
      "2m 11s (- 2m 11s) (5 50%) 7.2140\n",
      "2m 20s (- 0m 0s) (10 100%) 6.9638\n",
      "2m 30s (- 2m 30s) (5 50%) 6.2654\n",
      "2m 39s (- 0m 0s) (10 100%) 5.4505\n",
      "2m 47s (- 2m 47s) (5 50%) 4.8459\n",
      "2m 57s (- 0m 0s) (10 100%) 4.4895\n",
      "3m 8s (- 3m 8s) (5 50%) 4.3482\n",
      "3m 16s (- 0m 0s) (10 100%) 4.3137\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "\n",
    "    n_iters = math.floor(int(len(variables)/batch_size)) #put this in an outer loop for n_iter\n",
    "\n",
    "    for n_iter in range(1, n_iters + 1):\n",
    "\n",
    "        #print (\"training pairs: \", training_pairs)\n",
    "        #print (\"training variables: \", training_pairs)\n",
    "        start_ix = (n_iter - 1)*batch_size\n",
    "        end_ix = n_iter*batch_size\n",
    "        batch_vars = variables[start_ix:end_ix]\n",
    "        inputs = [variable[0].data for variable in batch_vars]\n",
    "        input_variable = Variable(torch.stack(inputs, 1).squeeze(), volatile=False)\n",
    "        targets = [variable[1].data for variable in batch_vars]\n",
    "        target_variable = Variable(torch.stack(targets, 1).squeeze(), volatile=False)\n",
    "\n",
    "        criterion = nn.NLLLoss()\n",
    "\n",
    "        # Run the train function\n",
    "        loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "        # Keep track of loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if n_iter == 0: continue\n",
    "\n",
    "        if n_iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (%d %d%%) %.4f' % (time_since(start, n_iter / n_iters), n_iter, n_iter / n_iters * 100, print_loss_avg)\n",
    "            print(print_summary)\n",
    "\n",
    "        if n_iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training loss\n",
    "\n",
    "Plotting is done with matplotlib, using the array `plot_losses` that was created while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1210002b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121989320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the network\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets. Instead we always feed the decoder's predictions back to itself. Every time it predicts a word, we add it to the output string. If it predicts the EOS token we stop there. We also store the decoder's attention outputs for each step to display later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this code doesn't work now that code is batchified\n",
    "#maybe just return the first entry in the batch\n",
    "#or return the result of a batch\n",
    "\n",
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variableFromSentence(input_lang, sentence, seq_len = MAX_LENGTH)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    output_words, decoder_attn = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
