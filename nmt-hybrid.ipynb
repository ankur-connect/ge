{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS-GA 1011 Lab 8 Neural Attention with Machine Translation\n",
    "\n",
    "Code adapted from http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from collections import Counter\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/carolineroper/Documents/School/Natural Language Processing/Char-NMT/data/train.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    DE_seq = []\n",
    "    EN_seq = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        DE_seq.append(line[0])\n",
    "        EN_seq.append(line[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "We'll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called ``Lang`` which has word → index (``word2index``) and index → word\n",
    "(``index2word``) dictionaries, as well as a count of each word\n",
    "``word2count`` to use to later replace rare words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = s.replace(\" <EOS>\", \"\") #there might be other special symbols I need to remove\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) #separates punctuation from the word\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) #strips anything that isn't a character of punctuation\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readLangs(seq1, seq2):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    seq1 = [normalizeString(s) for s in seq1]\n",
    "    seq2 = [normalizeString(s) for s in seq2]\n",
    "    pairs = list(map(list, zip(seq1, seq2)))\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    }
   ],
   "source": [
    "pairs = readLangs(DE_seq, EN_seq)\n",
    "\n",
    "input_lang = Lang('ger')\n",
    "output_lang = Lang('eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a *lot* of example sentences and we want to train\n",
    "something quickly, we'll trim the data set to only relatively short and\n",
    "simple sentences. Here the maximum length is 10 words (that includes\n",
    "ending punctuation) and we're filtering to sentences that translate to\n",
    "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
    "earlier).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#will ultimately remove this and set MAX_LENGTH to the longest sentence.\n",
    "MAX_LENGTH = 5\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "pairs = filterPairs(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 54 sentence pairs\n",
      "Trimmed to 54 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "ger 72\n",
      "eng 65\n",
      "total # pairs = 54\n",
      "avg # of german sentences = 3.8518518518518516\n",
      "avg # of english sentences = 3.9814814814814814\n",
      "['sie wurde geschlagen .', 'she s beaten .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(pairs, input_lang, output_lang):\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return pairs, input_lang, output_lang\n",
    "\n",
    "pairs, input_lang, output_lang = prepareData(pairs, input_lang, output_lang)\n",
    "\n",
    "\n",
    "print(\"total # pairs = {0}\".format(len(pairs)))\n",
    "print(\"avg # of german sentences = {0}\".format(np.mean([len(x[0].split()) for x in pairs])))\n",
    "print(\"avg # of english sentences = {0}\".format(np.mean([len(x[1].split()) for x in pairs])))\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trim_vocab(list_of_langs, vocab_size):\n",
    "    for lang in list_of_langs:\n",
    "        vocab = [count[0] for count in Counter(lang.word2count).most_common(vocab_size)]\n",
    "        for key in input_lang.word2index.keys():\n",
    "            if key not in vocab:\n",
    "                lang.word2index[key] = lang.n_words + 1\n",
    "        lang.index2word[lang.n_words + 1] = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trim_vocab(list([input_lang, output_lang]), 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    #will set n_layers to 4 to match architecture of Luong's paper\n",
    "    #but this may also be a hyperparameter we can tune\n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, self.hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "        \n",
    "    def init_weights(self):\n",
    "        #is it okay that all the lstm layers share an initialization or do I need to create separate layers\n",
    "        #for each round of LSTM?\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear_f, self.linear_i, self.linear_ctilde, self.linear_o]\n",
    "        em_layer = [self.embedding]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)\n",
    "\n",
    "    def initHidden(self):\n",
    "        #the paper says to use \"uniform initialization of parameters in [−0.1,0.1]: does that include these?\"\n",
    "        h0 = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return h0.cuda(), c0.cuda()\n",
    "        else:\n",
    "            return h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    #will ultimately set max_length based on the maximum length of a sequence found in our data\n",
    "    #changed dropout to .2 based on Zaremba paper\n",
    "    \n",
    "    #should there be more than one hidden?\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.2, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn_1 = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine_1 = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn_2 = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine_2 = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_output, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        cat = torch.cat((embedded[0], hidden[0][0]), 1)\n",
    "        \n",
    "        #first attention mechanism for the sake of the word-level model\n",
    "        #this was a softmax originally, replaced with tanh as paper describes\n",
    "        #but the actual attention mechanism may be different\n",
    "        attn_weights_1 = F.tanh(\n",
    "            self.attn_1(torch.cat((embedded[0], hidden[0][0]), 1)))\n",
    "        attn_applied_1 = torch.bmm(attn_weights_1.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        #the paper doesn't explicitly mention these layers\n",
    "        #but I think this is necessary in general for \n",
    "        #an attentional decoder\n",
    "        output_1 = torch.cat((embedded[0], attn_applied_1[0]), 1)\n",
    "        output_1 = self.attn_combine_1(output_1).unsqueeze(0)\n",
    "        \n",
    "        #second attention mechanism for the sake of the character-level model\n",
    "        \n",
    "        attn_weights_2 = F.tanh(\n",
    "            self.attn_2(torch.cat((embedded[0], hidden[0][0]), 1)))\n",
    "        attn_applied_2 = torch.bmm(attn_weights_2.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        output_2 = torch.cat((embedded[0], attn_applied_2[0]), 1)\n",
    "        output_2 = self.attn_combine_2(output_2).unsqueeze(0)\n",
    "\n",
    "        output_1 = F.log_softmax(self.out(output_1[0]))\n",
    "        return output_1, hidden, attn_weights_1\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromSentence(input_lang, pair[0])\n",
    "    target_variable = variableFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    \n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "   \n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "        if ni == EOS_token:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    " \n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MAX_LENGTH = 2836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"\n",
    "    # process input sentence\n",
    "    input_variable = variableFromSentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # encode the source lanugage\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "    \n",
    "    # decode the context vector\n",
    "    decoder_hidden = encoder_hidden # decoder starts from the last encoding sentence\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    # output of this function\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    # unfold\n",
    "    for di in range(max_length):\n",
    "        # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "        #error here\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "        # hint: print out decoder_output and decoder_attention\n",
    "        decoder_attentions[di] = decoder_attention.data\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        # stop unfolding whenever '<EOS>' token is returned\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "        \n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    \"\"\"\n",
    "    Randomly select a English sentence from the dataset and try to produce its German translation.\n",
    "    Note that you need a correct implementation of evaluate() in order to make this function work.\n",
    "    \"\"\"\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "   If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trying the tip about commenting out the init lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 37s (- 46m 58s) (1000 3%) 1.6434\n",
      "3m 9s (- 44m 13s) (2000 6%) 1.4737\n",
      "4m 32s (- 40m 49s) (3000 10%) 1.4855\n",
      "5m 46s (- 37m 29s) (4000 13%) 1.5011\n",
      "7m 4s (- 35m 22s) (5000 16%) 1.4545\n",
      "8m 25s (- 33m 41s) (6000 20%) 1.4947\n",
      "9m 45s (- 32m 4s) (7000 23%) 1.4847\n",
      "11m 4s (- 30m 28s) (8000 26%) 1.5031\n",
      "12m 32s (- 29m 15s) (9000 30%) 1.4840\n",
      "14m 10s (- 28m 21s) (10000 33%) 1.4570\n",
      "15m 39s (- 27m 2s) (11000 36%) 1.4454\n",
      "17m 9s (- 25m 43s) (12000 40%) 1.4704\n",
      "18m 31s (- 24m 13s) (13000 43%) 1.4362\n",
      "19m 55s (- 22m 46s) (14000 46%) 1.4421\n",
      "21m 17s (- 21m 17s) (15000 50%) 1.3763\n",
      "22m 49s (- 19m 58s) (16000 53%) 1.2815\n",
      "24m 13s (- 18m 31s) (17000 56%) 1.0741\n",
      "25m 31s (- 17m 0s) (18000 60%) 0.9571\n",
      "27m 3s (- 15m 39s) (19000 63%) 0.8719\n",
      "28m 44s (- 14m 22s) (20000 66%) 0.8363\n",
      "30m 13s (- 12m 57s) (21000 70%) 0.9069\n",
      "31m 28s (- 11m 26s) (22000 73%) 0.7329\n",
      "32m 46s (- 9m 58s) (23000 76%) 0.7576\n",
      "34m 9s (- 8m 32s) (24000 80%) 0.5649\n",
      "35m 44s (- 7m 8s) (25000 83%) 0.6002\n",
      "37m 19s (- 5m 44s) (26000 86%) 0.5531\n",
      "38m 44s (- 4m 18s) (27000 90%) 0.4143\n",
      "40m 14s (- 2m 52s) (28000 93%) 0.3908\n",
      "41m 47s (- 1m 26s) (29000 96%) 0.3501\n",
      "43m 13s (- 0m 0s) (30000 100%) 0.3755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1252f47f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvXmYXGWZ9/95au+q3td0J+l0dkJIIBD2JaAIBFTUGUcF\nkUEdBkZ98R2dUYdXRJkZncGZcQVkUBF05KegggPIIkuAsCWQfe2snaT3tbqrq2t7fn+cpau6q7ur\nk+qlOvfnunKl+tSpc57TlXzPfb7P/dy30lojCIIgzCwcUz0AQRAEIfuIuAuCIMxARNwFQRBmICLu\ngiAIMxARd0EQhBmIiLsgCMIMRMRdEARhBiLiLgiCMAMRcRcEQZiBuKbqxOXl5bqurm6qTi8IgpCT\nbNy4sU1rXTHWflMm7nV1dWzYsGGqTi8IgpCTKKUOZbKf2DKCIAgzEBF3QRCEGYiIuyAIwgxExF0Q\nBGEGMqa4K6XmKqVeVErtUEptV0rdlmaf65VSW5RSW5VS65VSp0/McAVBEIRMyCRbJgZ8SWv9jlKq\nANiolHpOa70jaZ8DwBqtdadSai1wP3DuBIxXEARByIAxxV1r3Qg0mq+DSqmdwGxgR9I+65M+8gYw\nJ8vjFARBEMbBuDx3pVQdsAp4c5TdPgM8PcLnb1ZKbVBKbWhtbR3PqW12NwX5j2d30947cFyfFwRB\nOBnIWNyVUvnAY8AXtdY9I+xzGYa4fyXd+1rr+7XWq7XWqysqxlxglZb9rb388IV62nojx/V5QRCE\nk4GMVqgqpdwYwv4rrfXvRthnJfAAsFZr3Z69IabicRn3o4FYfKJOIQiCkPNkki2jgJ8CO7XW/znC\nPrXA74AbtNZ7sjvEVLwuJwCRWGIiTyMIgpDTZBK5XwjcAGxVSm0yt/0TUAugtb4PuAMoA+4x7gXE\ntNarsz/c5MhdxF0QBGEkMsmWeRVQY+zzWeCz2RrUaHhNcZfIXRAEYWRyboWqeO6CIAhjk3Pi7hVb\nRhAEYUxyTtzFcxcEQRibnBV38dwFQRBGJufE3UqFlMhdEARhZHJQ3CVyFwRBGIucE3ePU7JlBEEQ\nxiLnxN3hULidSiJ3QRCEUcg5cQfDdxfPXRAEYWRyUtw9LodE7oIgCKOQk+LudTnEcxcEQRiFbPVQ\nVUqpHyil6s1eqmdOzHANJHIXBEEYnWz1UF0LLDb/nAvcywT2UDUidxF3QRCEkRgzctdaN2qt3zFf\nBwGrh2oy1wIPaYM3gGKlVHXWR2sikbsgCMLoZKuH6mygIennIwy/AWSlhypItowgCMJYZLWH6lhk\no4cqGAuZJHIXBEEYmYzEPYMeqkeBuUk/zzG3TQhet2TLCIIgjEZWeqgCTwCfMrNmzgO6tdaNWRxn\nCh6nTKgKgiCMRrZ6qD4FXA3UAyHgpuwPdRCv2ym2jCAIwihkq4eqBj6XrUGNhUTugiAIo5ObK1Td\nIu6CIAijkZPibmTLyISqIAjCSOSkuEvkLgiCMDq5Ke5OB5F4AsPqFwRBEIaSm+LudqI1ROMi7oIg\nCOnISXG3Wu1F4mLNCIIgpCMnxd3rNvuoRmVSVRAEIR05Ke4SuQuCIIxOTor7YOQu4i4IgpCOnBR3\nj9MJIOmQgiAII5BJ4bCfKaValFLbRni/SCn1R6XUZrMN34TWlQHwew1x7x2ITfSpBEEQcpJMIvcH\ngatGef9zwA6t9enApcB/KKU8Jz60kSn1G4fvCkUm8jSCIAg5SyZt9tYBHaPtAhSYpYHzzX0nNKQu\nMcW9MxSdyNMIgiDkLJmU/B2LH2HUcz8GFAAf01pPqBleHHADErkLgiCMRDYmVK8ENgE1wBnAj5RS\nhel2zFYP1QKvC5dD0dEn4i4IgpCObIj7TcDvtEE9cAA4Jd2O2eqhqpSi2O8WW0YQBGEEsiHuh4H3\nAiilqoClwP4sHHdUSvwesWUEQRBGYEzPXSn1a4wsmHKl1BHgG4Ab7BZ7dwEPKqW2YnRs+orWum3C\nRmxS4vfQKeIuCIKQlkza7H1ijPePAVdkbUQZUux3c7gjNNmnFQRByAlycoUqGJG7TKgKgiCkJ2fF\nvTjgpisUlYYdgiAIachZcS/xe4jEE4QiUvZXEARhKDkr7qX2KlWxZgRBEIaSs+Je7LdWqUquuyAI\nwlByVtzzfUaiTzAslSEFQRCGkrvi7jXEvU/K/gqCIAwjZ8U9YIl7ZPLF/Y7Ht/HNP26f9PMKgiBk\nSjaqQk4JVuQ+FQ07Nh7qxOvK2fuiIAgnATkr7oEptGV6B2LEE85JP68gCEKm5Gz46XdbrfbidPdP\nbsZMbzhGf1Ty6wVBmL6ccA9Vc59LlVKbzB6qL2d3iOlxOBQBj5MNBztY9a1n2d0UnIzTAhAciMni\nKUEQpjUn3ENVKVUM3AN8UGu9HPhodoY2NgGvi61Huklo2NfaOynnHIjFicQShEXcBUGYxmSjh+p1\nGM06Dpv7t2RpbGOS73URND331uDApJyz18yrD0XjE1bXRmvNkU6peCkIwvGTDc99CVCilHpJKbVR\nKfWpLBwzI6xJVYC23kkSd/NmEk9oIvGJaRX7xOZjXPRvL/LG/vYJOb4gCDOfbIi7CzgLuAajn+rX\nlVJL0u2YrR6qFgHvYMbKZEXuySti+01r5v/8+l0+fv/rWTvHkc5+AP64+RgA3aEodzy+bUrSPrNN\n70CMR946LNU8BWGCyYa4HwGe0Vr3mR2Y1gGnp9sxWz1ULfKnMHIH7IyZJzYf4439ozlX48PKoX/3\ncBcAT25t5KHXD/HGvrEj+UgswYd+/Bqv7D3xm+dE8OSWY3z1d1vZ2zI5cySCcLKSDXF/HLhIKeVS\nSvmBc4GdWTjumCTbMpPtuQOEInHiiexHoNbTwe7mIH0DMV7bZ3QtPNrVP+ZnD3eE2NTQNW0tnabu\nAfPv8BSPRBBmNifcQ1VrvVMp9SdgC5AAHtBaj5g2mU1SPfcTK/2bSGiUAqXUqPsFBwZz6vsjcY4l\nCa7WeszPZ0Kyr7+rqYfXzYjdEvdD7X3UlvrTnqvBbD1oieh0oyVoiHpTj4i7IEwkmWTLfEJrXa21\ndmut52itf2qK+n1J+9yttT5Va32a1vp7EzvkQfKHRO57m4NorYnEEnzrjzvGZdUs+KenuP0PY9+T\nhkbu9Un2QrZy35PP8creNrud4NGufvY0B1lz90u8PkJkbvWVbU4Sz3hC877/fJmH3ziUlfGdCC3m\nE1ZLBuLe1jvAGd96lncPd070sARhxpGzK1QBAh5D3D1OB5F4gvf91zpe2NXCjsYefvbaAf68szmj\n41gTo//z5mF72zPbm9h+rHvYvsEhnntyfn3yZOtzO5q5+5ld47sg+xxRu179a/WGJVNV6OVoZz/7\nzfPtG8GztiP3JPHc1NDJ3pZeXtqVeZZqW+8A7RMwj2GJe3PP2Mfe19JLVyjKnubJW6AmCDOF3BZ3\nM1tmQUXA3vbU1iZblBo6+nl801GC4Sj//L87ONyePnc8OcK3Shn87cMbueYHr9rbNx7qZHdTMCWq\n7o/EUsS9Jzxo2Tyx+Rg/ffVARlkh0SEplcFwjHmlfhwKth41bjCr60o52tVve9WN3WG6Q1HW3P0i\nmxq67M/akXuSp/3iLmNy1TrWSBxs67OP/5F71nPWPz/PQCy7i7VaezK3ZVrN76WnP/ezhARhsslp\ncbdsmUWV+fa2F3Y129Hh8zubue2RTXz3md088OoB/rS90d4vntCsuftFfruhIUXc39zfniJolth9\n7XdbuPOJ7SnZMm29EZ7b0YzLYXjfPUk1blqDYcLRxJjC9PTWRs6867mUMQTDMQrz3JTnewlHExTl\nuVlSWUBrcICD5g2qsTvMwfY+DrWHUiZPG8w0yuBAjBt/9hav72vnxd1GxN4SHBjVDrn0uy9x3rf/\nTDyh7ZvET189ABjzACeaiqm1tgU7E1vGmiRPvmmOxZd/u5lntjcd3wAFYQaR0+JuTai+f2UNv/zM\nufz4ujPpDEV5fodhx+wy6808v9MQt8akaLa9d4BD7SHW7W1LmYx9tb4tJfPmqa3GDaE1OMCOxh6C\n4ZidqviTdfto643wtauXAakiZB2jOTi6iFnH/NO2QUHqHYhR6HNTWegFoKY4j9kleQC8a0bpjd39\ndv/YQ6bga61p6AjZls7Le1q5/Q9b2X6sh/MWlAJG9P7CruZh5RqSnx5eTLJvnth0jEc3HuG0bzzD\nX//srVGvZSw6Q1GicWPiOhNbxvodZtptK57QPPbOEV7eMz3TQAVhMslpcS8NGE2yZxX5uGhxOSvn\nFAHw9sHUnHMryyR5ktESlx3Hum0b59TqQv68syVFeNbvayMWT9AZitLdb/i/lug2dPSzqraYy5Ya\nOfvJUbp1w0iX8heKxNjZ2JMypqe3DT5VBMNR8r0uqgp8AMwu9jHHFPfNtriHk8S9D4COvgi9AzFW\nzyu1j1WcZwj9ZUsrUQrePtjJLb98h28/lTofYEXqAHc9uQOA8xaUcrC9jwfXG9H7hkOdJIakfta3\nBO0b4FhY1zq/PEBr78CYaaSWuHf3R7nv5X32xPJIdIYiaG08QV37o1f58Yv13P77rdz087fs35sg\nnCzktLifv6CMX3z6HE43RX1WkQ+Hgp4RIr1kobU83/1tfTSYdVw+dvZcjnb12wuAFlYE2NkYTBGV\n7cd6qMj32j8vqSygwGcI6H89v4evPraFgdhgGWJL0B56/SDv+e5LaK2596V9rP3+K6zf12bfSF7f\n127fZHrDMQp8rpTIfXlNIY6kzMfG7jAdfcY5rMh9T7MRjV+8uNzez7Koakv9LK8p5H/ePEQkluD1\nfW1EYoPRujVBW13ks493xamzCEcT7DjWYz+tDH0S+f6f67ntkXdpDQ6MOfFpjWXl7CLiCZ0yYXu0\nq5+/uHd9yiS2ZeG8e7iT7zy9iyfHuIlY31N3f5SdjUE2N3TxqzcP8+LuVu55qX7UzwrCTCOnxd3h\nUKxZUmHne7udDmYV+kbcP1ncLdHVGl6tb6fA52LtabMA+PVbRtbMmiWVHO3qZ19rX8pxTq0ptF/X\nlvkpMJt1H2oP8fS2phSbxzrPuj2t7G/ro6MvYkfJ//DbLRzr6mdOSR4JDc/uaCae0PRF4uT7XFTY\nkXseBT43p8wyzpvndhKJJTjQZgjyse5+BmJxW1zXLBlc/WuVMigNeLh4cYV94+uLxNlwaPAJp960\nae6/YbW9bemsAgASGq5YbvxuDgz5XWw50kU0rrnxZ29xzQ9eob5lZIG3fPZVtSUA9k0V4E/bmth4\nqJOP3LPevjFakbv1+2od4tO/ub89xQprT3paisQTKWmqx7okr144uchpcU+H5U37PcMzaVqChhVw\nqL0vRVg2N3RRke+lstDHKbMKaO4ZwOlQXLS4DDCsmWS+ePlg6Zw5JXn43E48ZmTb3R9NycqxIvPt\nxwwbpqGzn66QIUhHu/rZ29LLJUsqmFfm56mtjfakZb7XRWXBYOQOsLjKmDi2bi47zGNqbYj47uYg\nRXlu5pX5ef1r7+GaFdX2OMryvXZEP6ckD7dT8dTWRho6QnSFItS39FJV6GXFnCK+97EzuP+Gs6gr\nH/zdve/UKgAOtA+Ke1coYkf5Oxp7iMY1dzw+2Fu2PxJn46HBHPVjXWGUggsWGr/XV/e2c8vDG+nu\nj/LWAWNSOBJPcN/L+4BBcbfcm9akSL9vIMZ1D7zJL5Ny963I/ZB5M9jfZoy10OfKaHWvIMwkcrbN\n3kjMLs7jbTo5f0EZOxt7uOG8eXzzjzuoKfJxrDvMpoYuPvaT14klNJUFXhxK0dQTpizf8O9X15Ww\nqylIacDD8hrD7nllryHud127nKWzCilPsmVqS/0AxJImJLceNfxdhzLsn46+iD2Ze6QzRGN3P3NL\n82joMARnVqGPq1dUc/+6/bZ/XuhzU1VkRO7zTZG1Iulqc/uupAYlB1r72NscZGlVAUopqovyqCgY\nHGdZwENtqfGUcdnSSuJa86s3D/PLNw5zxalVNPWEWVhh3Dw+tGo2YKza9bocDMQSXLK4HK/LkRK5\nbzliWChKGTeYM+YWs35fO7ubgiydVcCnfvYmbx/sZPMdV+B2KY52hajI9zK/PIDbqXjg1f0EwzHW\nrpjFmwc6+KvVcxiIJfj5awe48fw62od47C09xkK1+eUBWs0btfU7BOjoM8Q/2W4COGteCS/ubqU/\nEifPI+0RhZODGRu5L6rKZ/3X3st159Zy7Rk1fOqCOgC+9/weYmYoOKvIEFUAn9m276x5hmXQ3R+l\nssBLacBj55F/8IzZnDO/NPl0zDXFPXlu0BK9hRX5tPSEU3zkho5+GrvCXLy4AqdpolcVevnIqtnE\nE5qfv3YQgHyfi0sWl/PYrRdw2mzjJvPpC+fzhfcs4p/M7JxQJM7SqgKK8tzc81I9u5uCdnQPgxPO\nToeiKM+Nx+Xgj5+/iH+8aim3X72MxWYK6bM7mtl2tJvVdanX5nAo5pcHqCr0Uuz3UFcW4GBS5G7l\nzb/3lEqcDsV3P7oSp0Px+Kaj9A7EePugEbX/ZN0+zvmXP7OjsYea4jxcTgfzygJ2Fswv3zhEVyjK\neQvK+OLlSwhHE9z7Uj3xhMbjHPwnuqOxh6u+/wq/evMw7aaQN3YPivvQm4GF9Z2OFr1vOdI17AlN\nEHKZmSfuxYbYlpnC5nU5+f7HV9lWgBWFgyF+7z/dEHfLn7UyTSKxBEopzphbDIDbqSj0DX/Qsc6T\njCXup80u4mhX2LZkfG4HOxt7CA4Yi5QWmpZRZaGPxVUFnDO/lN+/exSAAp8LpZQtTMbnnXzpiqXU\nFOfZ551Tkse3rl3OO4e76AnH7KcN6/oASvweHOaNpK48QIHPTcDr4k+3XcJ9nzwTMH1103pJ5sOr\nZvNXq+cCcEp1Aa/sbeM3bzcAhp01vzzAHe9fzk8+eRaLKgu4cFE5j7zdwE0/H0ybfNK0m7Yd7bFv\nvguSLJ+3D3bidiouWVLB/PIAFy4q49dvGedYMmvwZtXYHSae0Kzb05o2G2mkbBrL4z82irjf/cxu\nvvLYlhHfF4RcIys9VM39zlZKxZRSf5m94Y0fSzzKAt6U7XNK/Pbr//OeRYDhh6+aW8yHzqjhXz+y\nwtzP+PzVK4wJxLPNaFah0hbqSrftcEeIojw3Z80roa13gF+/dZiFFQEWVxbYaZo1xXksNSdIrZTH\nT19YZx8juW5OOqy5hGK/hw+eXsMjN5/HLz59Dh9dPcfex7oBlOcPvwGBEZmfv7Acp0NRU+RjedJE\nscXfrlnIl65YCsDt1yzjzNoSbv/DVho6Qmw50s3KOUXUlvm53LwxfOl9S6gp9tHcM8DnLlsIDGbz\ngGGbGePPN8dvZBp9YGWNbXd9/OxaIvEEly2t4AMra4aN6a0DHfbkbLJgp4vcy/O99tzBaJH7sa5+\nGjr6Z0TNfEGAzDz3B4EfAQ+NtINSygn8G/BsdoZ1/KycXcQZc4s5MyniBSOKff7vL6G6KI9wNM4P\nXqhneU0hSim+9/FV9n5KKXZ860rcph1wznzjOEO7Lt33yTNtewfg+nNr2dTQRVN3mPa+CCtmF3HZ\nKZWAIW63XrqQQ+19tpVRU+zjtJpCntxyzPbQr1w+i3PqSnnrYEeKr5+O+eUB3j7YSYnfjVKK8xaU\nDdvHitxL0zxdWBTlufnkubXUlQfGrGhZWeDjvz52BmvufpH/94dtNPWEWTG7KGWf0+cW879fuBgw\nFlX99NUDhKODvztL3K2nllvXLOR7z+/lMxfPt/e5ZkU1bqfi4sUVadMfgwMxe6FSTzhG30CMTQ1d\naevtVBf5qCrwohT8/p2jXLSo3LbSkrEmvvc0BzmztmTY+4KQa4wp7lrrdUqpujF2+wLwGHB2FsZ0\nQpQEPPzhcxemfW9RpTEhGfC6+NMXL7YnQ4fi9wz+WlbMLk67z1WnVaf8/C8fNiL/BV97EoBrVlYz\nuziPU2YVsKspyJXLZ6WsQq0uyuOG8wtZVVtCiSm+Sin+52/OZVdTMK0AJWNFviWjCLc1SVw2xo3i\nm9eeNur7ycwq8nHjBXXcv24/YIj5SCilqCnOY3/SJKyV+XPNymrCsQTXn1PL31y8wLaNwHiisH6/\nlhVW4HMRDMdYUB5gf1sf6/YM2mvfeXqXXfEy4HHSF4nj9ziJxTWziny4nA4KvC7eOtjB3zy0gadv\nuzjlRhYMR+2IfXdTenGPxBL0R+IUmU8agjDdOWHPXSk1G/gwcO+JD2fyOGVWYYqIj4TH5eCLly/m\nux9N21xqGFYwf6WZF/5Xq+dy+pwiVs4u4qYL61hWXUh1kY/KAi9+j2vYBK3L6bAnUEfDyqApHkVs\nSvymuI9yAzgeblmzkHyvC4cirZWTjBWpl5jjrCk2nlL8Hhc3nDcPh0OlCPtQCs0FYqdWG+c5d0EZ\nlQXelCeph984RJW54KvPrPBZ4vdw+amVXLTISP+85/qz+NT589jVFOSlIeUJklcu725Kn6d/38v7\nuPJ766Q9oJAzZCMV8nvAV7TWibEe65VSNwM3A9TW1mbh1JNDcl77WPzi0+ewr6XXtkI+fdF8Pn2R\nYTlUFfp4+raLs9LU47TZRXicDjt9MR3Ffg+VBV6WVBWc0LmGUhrwcPs1y9h+rHvMG6Q1h/HZixfw\nWn3bqONNR6FZPmHF7CLePNDB0qp8jnb10xJsparQa9spj95yAR++Zz1/tXoO97y0j2K/m3uuP8s+\nzkWLyzl3QSnP7WjmofUHuWxppf2e1djE7VR2WQiLxzcd5fV97fQOxOy01rGehARhOpANcV8NPGKK\nVTlwtVIqprX+w9Adtdb3A/cDrF69ekaGQGuWVKSsEE1HNro1zS7O49073pfSjWooTofita++x65a\nmU0+cU5mN+eaIkPcP3h6DZ+7bNG4zzO7OI+iPDdXLJ/FuQvKuHBRGY3dYdbtaeW0miKae1q44tQq\n5pb62fD/LqezL2KL+1DcTgdXLp/FI28fJhyN2+mvVimKSxZX8Ep9G6FIzL5pPb+zhae3Ntp1iw51\nhETchZzghMVda23PhCmlHgT+N52wC9lnNGG3cDunNtv1L86ag8/ttCP48VIS8LD5G1ekbDul2ngS\nmVXk463b35uSGWWVgij2p7eiLllSzoPrD/L4pqOsWVLJrCIfTWau/PXn1fLnXS2s29PGVWYpitZg\nmFhCs8OM6Bs6QjLhKuQEJ9xDdUJHJ+Q8NcV5/M0lC7J6zKVVhv9elu+lsiC1lpDL6aAoz035CPMM\n5843soq+8thWllUX8tmL5vPdZ/cQ8Di5eHEFXpeDW365kcuWVvDTG8+2SyBYGT+HRmj4IgjTjUyy\nZT6R6cG01n99QqMRhAxYXJXPmiUV9sK0odx/w1kjZhsFvC4+etYcfrvxCDsbe7jzCaMWjs/txO10\n8Fer5/L/bWjgxd2tw4rAQWppZEGYzqipmv1fvXq13rBhw5ScWxC2He3m/T802ije9t7FrFlawZm1\nJWitSWhY+/11JDQplSUBzqkr5Te3nD8VQxYEAJRSG7XWq8fab8aVHxCETFheU0hVoVE76PPvWWT7\n6EopnGae/VBhn18e4FBHX7rDCcK0Y8ZVhRSETFBKcecHlqNJP+m8uHJ4yuZZ80p4dOMRovHElE9U\nC8JYyL9Q4aRl7YpquyroUJLXBtSY5SGsdMjkHruCMF0RcReENMwvD9glmS8/tYpTZhUw1yw+19Qj\nXZ2E6Y/YMoKQBo/LQV2Zn32tfdx+zTK8Lqddl79FxF3IASRyF4QRWFJVQLHfjddlrGS1+vNaJQ8E\nYTojkbsgjMDnLlvE+5PqyZf4PbidSmwZIScQcReEEThtdlFKhU6HQ1FZ4EupIikI0xWxZQRhHBiV\nKEXchemPiLsgjIOqQp947kJOcMI9VJVS1yultiiltiql1iulMutqIQg5SFWhj+ZuidyF6U8mkfuD\nwFWjvH8AWKO1XgHchVmvXRBmIqUBD8GBGNEhPXUFYboxprhrrdcBHaO8v15r3Wn++AYwJ0tjE4Rp\nh1Uvvs/suToW4Wich14/SDwxI3vTCNOYbHvunwGezvIxBWHakG82SAmGMxP3/163nzse385j7xyZ\nyGEJwjCylgqplLoMQ9wvGmWfnOyhKggWBWbD7kzFPWZG7IfapZqkMLlkJXJXSq0EHgCu1Vq3j7Sf\n1vp+rfVqrfXqiorR+4wKwnTEsmV6M7RlyvKNjlDtQ5p+CMJEc8LirpSqBX4H3KC13nPiQxKE6Ysl\n7sFwNKP9La+9rVfSJ4XJJZNUyF8DrwNLlVJHlFKfUUrdopS6xdzlDqAMuEcptUkpJe2VhBmL5bmn\ni9xf2dvKWXc9lyL8/dE4AK0SuQuTzAn3UNVafxb4bNZGJAjTmHwzcu8Jx6hv6eWPm49xy5qF5Hmc\nbDnSTXtfhOaesO3NhyOGuLdJDXhhkpHaMoIwDgpN0e7pj/Kl32xi85FuHt90lPMWlOFyGvXfu/sH\no/pwzMiHb+oJE4sncEkHJ2GSkH9pgjAOvC4HLofitxsa2Hykm+vPrcXndvLI2w1sPNQFQE+yLWNG\n7vGEplFWtgqTiIi7IIwDpRQFPhcH20P4PU7uuvY0vvGB5QDsbuoBjKjewvLcAY529U/uYIWTGhF3\nQRgnlu8+t8SPw6GoLTPa71mLUEcSd8mYESYTEXdBGCcFXsN3n1OSBxgdmtym3w7GZKvFQDROtdlg\nWyZVhclExF0QxomVDjm31IjYnQ5FTXGe/f7QyL26yIfToWiVyF2YRETcBWGcJLThv1iROxgWjcXQ\nCdWA10VpwENbUHLdhclDxF0QxkmXGZmniHtpcuQ+aMv0RxN4XU4q8r209g6w5UgXWk9shchILEEk\nJiWJT3ZE3AVhnHSFLHEfjNat16UBT0rkPhCNk+dxUl7g5YVdLXzwR6/x550tEzq+0+58hvf+50sT\neg5h+iOLmARhnFi2TLIV88HTa+iPxNl8pGuY557nduB2euxtwYHM6tIcL5FYgoYOSbs82ZHIXRDG\nyS9uOoe/u3QhhXmDsdHcUj9fvnIpxX5PSraMIe5OKgq89jaHUgjCRJONHqpKKfUDpVS92Uv1zOwP\nUxCmDysIfgR2AAAfzElEQVTmFPGPV52CSiPShT5XauQeiePzGJ67Rd9AfNjnJoKEdH86qclGD9W1\nwGLzz83AvSc+LEHITQrz3PSEo2itSSQ0A7EEPpfTtnIAQpHMasGfKB0hyc45mTnhHqrAtcBD2uAN\noFgpVZ2tAQpCLlGU5yYa14SjCQbMjJU8j5PTZhfZ+4wUucfiCb7+h20caMtO16aWHsmrP5nJhuc+\nG2hI+vmIuU0QTjqsqpFNPWG79ECe28kFC8vZcucVeF2OESP3/W19PPzGIX6zoSHt+5mQ3Ii7JSiF\nyk5mJnVCVSl1s1Jqg1JqQ2tr62SeWhAmhQsXleF1Ofj2UzvpMxt65LmdgCH8Aa+LvhHE3SostuHg\naA/KozMQG3wqaAkOEIsniMYl5/1kJBvifhSYm/TzHHPbMKSHqjDTmVcW4O/ft4RndzTzF/euB8Dn\ncdrv+z1OQqYtE47GuePxbXaEfcwU980N3YSjxzfpGo4OCnlrcIAr/msd53/7zzy5pZFfv3X4uI4p\n5CbZEPcngE+ZWTPnAd1a68YsHFcQcpKbL1nA7Vcvo8UsFOZzDf43C3gGI/c39rfz0OuHeHZ7MzAo\n7pF4gm1Hu4/r3Mk3hZaeMPvb+mjrjfCjF+u56393HPdNQ8g9stFD9SlgP1AP/DfwdxM2WkHIAZRS\nfOqCefbPecmRu9dJyGzgYQl4fUsvAMe6whSYRcm2ZkPck6pQ7mrqIRSJ8+KuiV0dK0wfstFDVQOf\ny9qIBGEG4HU5cTkUsYS2PXcwI3fTi7cEfF+rIe5HO/tZVl3IpiNdNB1n16ZkW6Y9qSm3lYn5v1sa\nWbsie8ls3aEo97xcz5evWIpbWghOK+TbEIQJ4r3LKgFIXkrk9zhp641w5xPbeca0Y/Y2m+Le1c/s\nkjxmFfpo7A6jtR53kbGwOaFqnCc1FXLlnCI2NXQd59Wk55X6Vn7y8v7jtpGEiUPEXRAmiLs/ejpf\nueoUzqwtsbfle10c7gjx4PqDAJT43TT1hOkORWnqCTO72BD3Q+19rP7n51n7/VdsLz4TLFumushH\nQ2fI3u73OLl0SQXHuvtTMmpOFMtiSi65IEwPRNwFYYIo9Lm59dKFOB2DZQr83kGLxuN0cNOF8wFY\nv6+NeEJTU5xHVZGPrUe7ae+LsKspOK689wHTlqkpziMaH4z6z5pXQl15AK3hSGf2ioqFTIspueSC\nMD0QcReESSTgGZzm2vrNK3j/SsP/fnpbEwDzywNUF/lILguT7J2PhRW51xQN1pf/8XVn8pMbzmKe\n2ev1cHso7WePh5B5vm4R92mHiLsgTCJ+U9xLAx68LifzygL43A6e32n474sq86kqNHquuhyKujI/\n7X2ZlxGwPPfqYp+9rarQi9/jorY0AMCh9uyUNwDsnP3kGvbC9EDEXRAmkYBpyxTnGWUKnA7F0lmF\nhCJxiv1uyvM9zDLFva48QGWhj7ZxRe6mLZMUuReZ5yrP9+D3OLnzjzu46edv0dY7wK6mHpq6w2w+\nzolWy3OXyH36Ic06BGESsSL3QlNwAU6tLmBzQxeLKvJRSjGryCgPvLgyH6Vgd1Mw4+PbtkzxcHFX\nStli/NKeVm795UbePthp73fwO9eM6zwv72mlP2p57jKhOt2QyF0QJhErci9KEvdl1YWAYckAti2z\nqDKfsoCXjr7MIvdntzexfl87kGrLJN9ILl5cDhh578nCDoxr9epzO5r524c3sv1YD5DdCdVtR7vp\nzPCahZERcReEScSq2jiauM8uzuPzly3iI2fOoSzfQ2coSmxI8S8jT74pZdu3n97FczsM796yZbwu\nB76kRVT3fvIsXvvqe3CZGTz3ffIsvvORFQDD8uJHw/LYrcyboZ77wRMoW/zJn77JA6/uP+7PCwYi\n7oIwiVgRbrK4nz6nmL++oI5rzMwZpRRfvnIp88sDlAWM3qvJjTdi8QQPv3GIJzYfSzl20BRYj8tB\nnseJ3+NMOQ8Yefazi/NYPrsIl0NxwaIyKgsNG6g1mLm495v2jvVUkey572rq4dLvvsQ7hzvTfnY0\ntNb09EftJuTC8SOeuyBMIteeMZs/bW/i1ksX2ts8Lgd3fnB52v3LzPZ87b0RKgt8rK9vw+dxEk/o\nYSmNQXMhkVWorMTvsW2godx88QLqW3op9LmpLDAsnJZxiLvl3Vsk2zKNZumE42kWEktoEhq7Fr5w\n/GQk7kqpq4DvA07gAa31d4a8XwT8Eqg1j/ldrfXPszxWQch5SgIeHrn5/Iz3tyL39t4Iu5p6uO6B\nN+3iYoc7DHHfdrSbQp/b7vxk9XYtDXjwudM/nFtPCYDdvHs8kftQcU+O3HvNm4w12ToeIuY1SPXK\nE2dMcVdKOYEfA+/D6LL0tlLqCa31jqTdPgfs0Fp/QClVAexWSv1Kay2zIoJwAliR+7Hufn6ybh8A\nQXNVaHd/lO5QlL95aAOnmr69tR3gi5cvTlkdO+I5Ah6UGq8tkyrcPeEYWmuUUvYTRH9k/E1CLHEf\nevMQxk8mkfs5QL3Wej+AUuoRjL6pyeKugQJlhAz5GD1XJTdKEE6Q8nwjcv/HR7ekfX9XUw+N3eGU\nssIW711WldE5XE4HZQEPrWNMqAbDUV7d28baFdXDxDee0PRF4uR7XfQOGDeX47FWIubEcb+I+wmT\nyYRqJj1SfwQsA44BW4HbtNbS20sQTpCiPDeXL6vk8mWVPPyZc/inq09Jef81M/XxROvFlOd7x/TI\nf//uUW791Ts0dYftsgMAhT4jRrR8dytyPx5rxaqNI7bMiZOtCdUrgU3Ae4CFwHNKqVe01j3JOyml\nbgZuBqitrc3SqQVh5qKU4oEbz7Z/jpnFwOrK/BxsD/HqXqMXsWVnHC8VBV5aewfY39rLO4e7+Muz\n5gzbxxL/tt6BlMh6VpGPnnAvXaEoNcV5triP1Ah8NCLxuPlZEfcTJZPIPZMeqTcBv9MG9cAB4JQh\n+0gPVUE4Qaxc+MVVBVQWeNl8JDt11CsLfLQFB/j8/7zLl3+7OW1xMavGTWcokiLcS2cZfr/VdKR3\n4Pg9d2tSWLJlTpxMxP1tYLFSar5SygN8HKNvajKHgfcCKKWqgKUYrfcEQcgis4vzKMpzs6A8wIWL\nyu1FUclkMok6lOoiH009Ybvw2OObhve4t2rcdIaiKZH7qrnFeFwOthwx6tNY+fbH5blLtkzWyKTN\nXkwp9XngGYxUyJ9prbdbPVS11vcBdwEPKqW2Agr4ita6bQLHLQgnJQ6H4onPX0hpwMMLu1r4/bup\nIvzYreczt9Q/7uPWlQeIJzSNXUaO+u/fPcrnLluEI+lGYS1Y6uyLpNgmhXluTq0utJ8irMj9uDz3\nmEyoZouMPHet9VMYjbCTt92X9PoYcEV2hyYIQjrmlRmle9csqcChjFQ1qxvfsupCuzjZeJhfbhyz\nPxqnusjH/rY+Hnm7gevOHZwbazezaTr6IilRecDj5PQ5Rfx24xHiCZ2UCnn8kXsoGrdTK4XjQ8oP\nCEKOUuz38E9XL+OmC4xuTg5FSjPu8WCJO8AtaxZy3oJS/v2ZXUTjwxtud4YiKcKd53Gyck4xoUic\n+pZeexFT6ARsGa0Ho3jh+BBxF4Qc5rMXL7BXm+Z7Xccd6Zb43XYdmtpSPzecV0dXKGpXfQxH4/bi\nqc5QlFAkjttpnCvgdbG6zugT+9bBDrufavh4Ivekm8lYtk5DR2hc/WVPNkTcBSHHKfYbolzgc4+x\n58gopagzo/e5pX5brDce6iSe0OxKqinf3jtAfzRulybOczupLfVTXeTjjf3tJ7SIKbl59+Yjo5f+\nvfjfX+SC77ww7nOcLIi4C0KOY3V1yvee2LKVBaa4zynJo6rQx+ziPDYe6uCBV/bzoR+/BhiZOE1m\nYbDLl1XxoTNqWFRpNBk5b0EZr+5ts7tBnUi2DMCNP3uLH75QP+ZnuqWCZFqkKqQg5DiWnZLvO7H/\nzh9aNZuiPLdd/311XQnr97Xb1gwYi6cazNWwCyoCfOr8wWqW5y8oS8neOZEJVYvmYHjMz7xa35ZS\nCE0wkMhdEHIcl9NBgdd1wpH7miUVKaWH155WTWtwgENJC5oWVebbAjx08vbSpYMLE31ux3HaMqni\nPlpU7jVLG7+8p2Xc5zkZEHEXhBlARYGXEv/xe+7puOq0WXzjA6eyel4J915/JqvnlbC8psh+f2ix\nsspCn93cuyzgPa7Ifai4d4bSe+7xhLb33TWOHrMnEyLugjAD+OF1q/jylUuzftybLpzPo7dewNoV\n1Tx66wVUFw32ZvWnqUR58yULACgv8NJv5qqPh6G2zEgdmfqSyh9YcwBTTTga54afvsme5ulxsxHP\nXRBmAMkR9URyzvxS+3Wee7h83HRhHatqi3l9fzubG7oIRxNpyxGPRCQ+VNzTR+5WLn1VoZfW4ACx\neAKXc2pj1SOd/byyt413DnWypKpgSscCErkLgjAOrNWxkD5yV0qxqrYEv+nHj9d3Hxq590XiaSte\nWiUOFlXmk9CMWYt+IumPxPnbhzfYhdOmy+IrEXdBEMaFVb/d4xpZPqxofbziPhCLDyt8ltzCz8Iq\ncbCowqiSOZXWzP62Xp7Z3sxr9UY5reRc/akkI3FXSl2llNqtlKpXSn11hH0uVUptUkptV0q9nN1h\nCoIwXfjex8+gKM/NnJK8Efex0inHmlSNxhO8/4ev8OSWRsCI3K3UTot01oxVedIqgdzcM3Xibq2k\nteYHrDz/qSYrPVSVUsXAPcBVWuvDSqnKiRqwIAhTy3tOqWLzN0avE2gVLxtL3Pe19rLtaA+/2dDA\nNSuricQS5HtddgVKgK6kyP0/nt3N/PKA/dSwsHLqI3dLzK1x5lLkbvdQNRteWz1Uk7kOo1nHYQCt\ntSSeCsJJjJUD3xGK8PU/bKNlhMh6h7lA6vX97YQiMQZiCTt/3cIqQbC3OcgPX6jn73+z2Z5QrS31\n43YqmsZoETiRWDcw6wljYJpE7tnqoboEKFFKvaSU2qiU+lS6AymlblZKbVBKbWhtbT2+EQuCMO3J\n8xjS8uSWYzz8xiGe3taUdr+djYa4R2IJXt/XTiSWwONy8NtbzuexW88HBiPi+9cZ/X/mlwfsCdXC\nPDeVBb5x2zItwTAv7c5ODGo1OLFsmXQTqlprrvnBK/xx87GsnDMTsjWh6gLOAq7B6Kf6daXUkqE7\nSZs9QTg5WFRZgMfp4LF3jHIEu5Nyv2PxBP/46GYe33SUnY1BllYV4HYq3j7YSSRuiPvZdaV2OqG1\nSvWlPUZAqBicUA14XMwq8o3blrnp52/z1z9/Oysdn4ZF7mlsmVAkzvZjPfbNbDLIVg/VI8AzWus+\nswPTOuD07AxREIRcoyjPzXuXVdptAHcnrSI92N7HbzYc4bZHNvFqfRsr5xQxt8TP4Y4+BmIJPGa+\ner7Xhcuh6AxF6BuI0Roc7OHaOxAj4HHidChKA54Ujz4TDncYJRWy4dWHzUg9aHegGh65943y3kSR\nrR6qjwMXKaVcSik/cC6wM7tDFQQhl/jwKsO9Lcpzs6cpSO9AjE8+8CZPbTUsmsuXVZLvdbFmaQXz\nyvwcbAsZnrvp1yulKPYbwm3Vt1lQEaC7P0pPf9QucVzq94xYpmAkrIycbNSDt+rWW4tx00XuduvB\nSZxszUoPVa31TqXUn4AtQAJ4QGu9bSIHLgjC9ObyZVX86LpVtAYH+OYfd/DrNw/zan0bO0xr4jt/\nsZLyfC8AGw528vbBTuaW+u3IHYzyww2dIQ619wFwxpxi9rf20dQTtqtgFgfcdIWi42rLV+x3c6Sz\nn2PZiNyHWDvpPHer5+xkNv7OSg9V8+e7gbuzNzRBEHIZh0Px/pU1bDjYAcADrxoToh19EfweJ2UB\nj73vvDI/vQMxmrr77bry1vYNBzs5ZNooK+cU8bt3j3Kks9+Ovkv8HiLxBKFInECGlTELvNmL3Icu\n1EqXLWNF7pOZSSMrVAVBmFBWzCliQXmA5qR0xdpSf0qUPa/MDxgt/JJXvs4rC9DY3c/e5l7KAh7m\nlBj7NXSE7BLHVjXM8fjusYQhslmxZYYIdjrrxfLcj6cM8vEi4i4IwoTidTm5+6MrcTsVp1YXAkYr\nv2SSa9Yk57nPK/WT0PBafRvzyvx2S8FYQtsVKkv8xhPASBUk09E3YIjs0SRxbwmGeej1g+OuZDlU\nzEeL3CfTlhFxFwRhwjlrXikbv/4+brt8MWBE7snMKcnDZdaUSY7c68qN/Zp6wiysyLfFHQZXp5aY\n9s54JlWtksHJkfuTWxq54/HtNI0zZ35oI/CRUiFBxF0QhBlIoc9tR+4LKgIp73ldTj5xTi0AB9r6\n7O21pYP7GW0AB336hWbRMMuWGZe4m5H7sa6wHalbAtzZN76erMMi9zQTqtM1FVIQBCErzC3189tb\nzucvzpwz7L0vXWGse7xy+Sx7W3n+oJhfsLAspajYQvMGUWzaMp3j8Nz7BmL4PU76o3EaOozofbAA\n2PjSKofWz0kn7lORCiniLgjCpHJ2XaldNTKZYr+H/f96NZ88b569TSnFXdcu55efORelVIplY/n2\nxabgv7C7lfX72lKOaWTgpNos8YSmPxrnwkXlALzb0AkMintnKErfQIxHNx7JyH8fNqGaxnrpk2wZ\nQRBOZhyO4XnqN5xfx0WLy4dtd5v58C6ng0Kfi3V7Wrnuv98EjFou0XiCbzy+nY/f/3rK50Km335m\nbQl5bifvHu4CBjNZOkMRntrayJd/uznFIhqJYamQaSP3aZrnLgiCMF3IczuH1ZLvCQ/2VNVa8w+P\nbuH37x6l0OeiMxQlHI3bTwuWt16Y52LFnCI2NZjiHjFL94Yi9r4dfREWjFEGa6hgR2KJYQuqrBuK\niLsgCMIIbPrG+1CMvBK1J2xYKmBYLGDkxS82C5FZFkm+18WqucX8/LWDROOJFFsm32vYMZmkV6YT\n7IFYIsV6sidUJ7EFn9gygiDkFF6Xc1iLv+vPrbVfN3b3D0u1tGrTwGCmjN/jYk5JHpF4gs6+SJK4\nR+yqk11pWvwNJV0GzFBv3ZpQjScMu2gyEHEXBCHn+ZcPr+CxWy8AoLErTFcogsuh+ODpNQB2+QIY\nzHEPeJyUBozaNu19Eds77wpF7TZ+mWTO9EcH+75aTszQXHfrhgKTZ81krYequd/ZSqmYUuovszdE\nQRCEsakpNlasHmjroycc44uXL+b7Hz+DfK+LhmRxN6PogNdFaWAwjbI/TeSerjn3UMLRuJ2xU2CW\nRBg6qWrdUIz9p0nkntRDdS1wKvAJpdSpI+z3b8Cz2R6kIAjCWFQW+HA6FFuOdNk/K6WoLfXbVSUB\n+swJ1YDXSZmZR9/eF0lquhElOGCI+lgLoxIJzUAsYa+cLTL/Hh65x+wVuNMpcs+khyrAF4DHAOmf\nKgjCpON0KGYV+th8pBuAikLDcqkt9afYMqE0kXtHX8SOtlM8d3NCVWudtouS9Rmrvk2xuYJ2aHTe\nNxC3zzWdxH3MHqpKqdnAh4F7RzuQ9FAVBGEiqS7y2bnpFWat+CVV+Rxs67MnNa2//R4XJX4PSqVG\n7t39UduOsf5+cP1B1n7/Fbt8sYUl1Hbknjc8ctda0xeJUWaOZ9rYMhnyPeArWutRRy09VAVBmEis\ndEeASjNyXzWvhISGLWY+u5XnbrXpK85z09E3QH80jkMZHZWOdBolCazI3WpsbbX6s+i3xd2Iym1x\nTxLw1uAAWsNsc05gskoQZKuH6mrgEaXUQeAvgXuUUh/KyggFQRAyZO1pg3VpysxMmDPnlgDwzmGj\nzEBXKIrf48RlrnC1erD2R+N2SQOr96vlue83nwaGTrBakbtVvKwwb/iE6i6zf+zpc4pTPjPRZKWH\nqtZ6vta6TmtdBzwK/J3W+g9ZH60gCMIoXLCwzH5tpScW+d0sqszne8/vZX19G0c6Q8wuHlzhWhbw\n0haMEIklWFxZkHK87pBh0VgRfPuQ4mTWU0BNcR5KQU2RcdxkW2ZPsyHuK+ca4r7taE/assDZZkxx\n11rHAKuH6k7gN1YPVauPqiAIwnTA5XTwxcsX86nz56VsP2d+KbGE5roH3uRQeyilWUhJwG037VhS\nlW9vLwt4CA7EeHXvYDGyobZMW6/x84rZRTz5hYtZu6IaSPXVdzUFKc/32rbMv/1pF99+alc2LndU\nstZDNWn7X5/4sARBEI6PL16+ZNi2r1x1ClrDr986zO7mIOctKLXfKw14bXGvKvTZ2+eW+mnvi/Dg\n+gOU53vwupzDIvcWU+wrC3zUlvlp7DaOk1xMbE9zkFNmFeB1DZYjOH1uURaudHRkhaogCDOeojw3\nnzhncOrQ6sUKpDTqzkuqB3PKLMOieftgJx9eNZtZRT7ae1MjdyuSrygw/H0rJdLq55pIaPY0B1k6\nqyCl1swZ5jzARCLiLgjCScGiyny7PMDc0kHP3cqqAfB5nHbGywfPqOEzF83H73HysbPnUp7vsW0Y\ni9bgAAVeF3keQ7h9bif5XhetwQH+9amdvHO4k3A0QW2pH587qX1gWWrtm4lAqkIKgnBS4Pe4mFvi\n53BHKCVyT/bffS4HJX433f1RCn1uvv7+U/nKVafgcTkoy/ey4WBnyjFbgmF7sZRFacDDliNdvHO4\ny74ZlAY8KZF7cjngiUIid0EQThqWmHnwc5PEPbmCZJ7HyQVmhya/GY1bFSjL8710hCLEkqo6tvQM\nUFmQKu5l+R52NhoZMvUtvYAh7lZzkRvOS53snSgkchcE4aTh4sXlHOkM2TVggJS0yDy3kzs/sJxr\nT69hQUV+ymfL8z1obdR7tzz21t4BVpr56xZlAS/9UWPBlCXulhe/71+vJk2zqQlBxF0QhJOGGy+o\n48YL6lK2JdslPrdRK/7cBWUMxVoUdayrnx+/WM+z25s41h3m8mVVKfslN/W28uCtujLOyVJ2RNwF\nQRBs0jXutlhqZs/8w6Ob2dPca2+vKBjuuQ+lOOlJYbIQz10QBMHEynpJx6LKfM6dX8qe5l7OrC22\nI3aPM1VGrQJhFgGPc9SbxkQhkbsgCCc9p8wqYFdTcJhQD+WmC+fz5oEObr5kISvnFNHTHx3VlgEo\nSRPJTwYi7oIgnPQ89OlzeG5n8zCLZShXnTaL5/7vJXb1yd/ccv6wfSxvfvDnqRF3sWUEQTjpqSz0\ncf25maUoJpcVTofV3WlOiZGFM1WRe1Z6qCqlrldKbVFKbVVKrVdKnZ79oQqCIEx/qot8RsbNfCPj\nptQ/TcU9wx6qB4A1WusVwF3A/dkeqCAIQi5Q7Pew7h8u46YL64Dp7bnbPVQBlFJWD9Ud1g5a6/VJ\n+7+B0dBDEAThpGRWkc9erFQyBWmQkKUeqkP4DPB0ujekh6ogCCcLFQVe/u/lS3j/ypopOX9Ws2WU\nUpdhiPtF6d7XWt+PadmsXr1aZ/PcgiAI0wmlFLddvnjKzp+JuGfSQxWl1ErgAWCt1ro9O8MTBEEQ\njoes9FBVStUCvwNu0Frvyf4wBUEQhPEwZuSutY4ppaweqk7gZ1YPVfP9+4A7gDLgHrNOcUxrvXri\nhi0IgiCMhtJ6aqzv1atX6w0bNkzJuQVBEHIVpdTGTIJnWaEqCIIwAxFxFwRBmIGIuAuCIMxARNwF\nQRBmIFM2oaqUagUOHefHy4G2LA5nKpFrmZ7ItUxP5Fpgnta6YqydpkzcTwSl1IaZkmop1zI9kWuZ\nnsi1ZI7YMoIgCDMQEXdBEIQZSK6K+0yqFy/XMj2Ra5meyLVkSE567oIgCMLo5GrkLgiCIIxCzon7\nWP1cpztKqYNmr9lNSqkN5rZSpdRzSqm95t8lUz3OdCilfqaUalFKbUvaNuLYlVJfM7+n3UqpK6dm\n1OkZ4VruVEodNb+bTUqpq5Pem5bXopSaq5R6USm1Qym1XSl1m7k9576XUa4lF78Xn1LqLaXUZvNa\nvmlun7zvRWudM38wqlLuAxYAHmAzcOpUj2uc13AQKB+y7d+Br5qvvwr821SPc4SxXwKcCWwba+wY\n/XY3A15gvvm9Oaf6Gsa4ljuBL6fZd9peC1ANnGm+LgD2mOPNue9llGvJxe9FAfnmazfwJnDeZH4v\nuRa52/1ctdYRwOrnmutcC/zCfP0L4ENTOJYR0VqvAzqGbB5p7NcCj2itB7TWB4B6jO9vWjDCtYzE\ntL0WrXWj1vod83UQ2InRBjPnvpdRrmUkpvO1aK11r/mj2/yjmcTvJdfEfbz9XKcjGnheKbVRKXWz\nua1Ka91ovm4CqqZmaMfFSGPP1e/qC0qpLaZtYz0y58S1KKXqgFUYUWJOfy9DrgVy8HtRSjmVUpuA\nFuA5rfWkfi+5Ju4zgYu01mcAa4HPKaUuSX5TG89oOZnClMtjN7kXw/I7A2gE/mNqh5M5Sql84DHg\ni1rrnuT3cu17SXMtOfm9aK3j5v/1OcA5SqnThrw/od9Lrol7Rv1cpzNa66Pm3y3A7zEevZqVUtUA\n5t8tUzfCcTPS2HPuu9JaN5v/IRPAfzP4WDytr0Up5cYQw19prX9nbs7J7yXdteTq92Khte4CXgSu\nYhK/l1wT9zH7uU5nlFIBpVSB9Rq4AtiGcQ03mrvdCDw+NSM8LkYa+xPAx5VSXqXUfGAx8NYUjC9j\nrP90Jh/G+G5gGl+LMvpa/hTYqbX+z6S3cu57GelacvR7qVBKFZuv84D3AbuYzO9lqmeVj2MW+mqM\nWfR9wO1TPZ5xjn0Bxoz4ZmC7NX6M/rN/BvYCzwOlUz3WEcb/a4zH4iiGJ/iZ0cYO3G5+T7uBtVM9\n/gyu5WFgK7DF/M9WPd2vBbgI49F+C7DJ/HN1Ln4vo1xLLn4vK4F3zTFvA+4wt0/a9yIrVAVBEGYg\nuWbLCIIgCBkg4i4IgjADEXEXBEGYgYi4C4IgzEBE3AVBEGYgIu6CIAgzEBF3QRCEGYiIuyAIwgzk\n/weQuyy9fJ4ScgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1252ef7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "###the paper says to use 1024-dimensional embeddings, so I guess the hidden size is 1024\n",
    "###6-epoch training with plain SGD and a simple learn-ing rate schedule – \n",
    "#start with a learning rate of 1.0; after 4 epochs, halve the learning rate every 0.5 epoch\n",
    "#how do we change the learning rate schedule and do I need to start an epoch counter?\n",
    "#it also says the batch size of 128, although I would say that this model currently\n",
    "#doesn't have a batch size or has a batch size of 1.\n",
    "n_layers = 4\n",
    "\n",
    "\n",
    "#not sure about n_words, could be vocab size.\n",
    "\n",
    "encoder1 = EncoderRNN(vocab_size, hidden_size, 4)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words,\n",
    "                               1, dropout_p=0.2)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 30000, print_every=1000) #75000 is the number of iterations, could recode this\n",
    "\n",
    "torch.save(encoder1.state_dict(), \"saved_encoder_3.pth\")\n",
    "torch.save(attn_decoder1.state_dict(), \"saved_decoder_3.pth\")\n",
    "\n",
    "#encoder1.load_state_dict(torch.load(\"saved_encoder.pth\"))\n",
    "#attn_decoder1.load_state_dict(torch.load(\"saved_decoder.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 66,\n",
       " '.': 5,\n",
       " 'ab': 66,\n",
       " 'am': 12,\n",
       " 'and': 15,\n",
       " 'ansteckend': 66,\n",
       " 'are': 3,\n",
       " 'auftragsarbeiten': 66,\n",
       " 'beaten': 36,\n",
       " 'besorgt': 66,\n",
       " 'bewegen': 66,\n",
       " 'biased': 55,\n",
       " 'bin': 66,\n",
       " 'chocolates': 62,\n",
       " 'christen': 66,\n",
       " 'christians': 20,\n",
       " 'clever': 66,\n",
       " 'colorblind': 64,\n",
       " 'commissioned': 63,\n",
       " 'critical': 17,\n",
       " 'crushed': 4,\n",
       " 'curious': 22,\n",
       " 'delicacies': 38,\n",
       " 'delicious': 46,\n",
       " 'delikatessen': 66,\n",
       " 'die': 66,\n",
       " 'domestics': 48,\n",
       " 'drei': 66,\n",
       " 'dynamic': 56,\n",
       " 'dynamisch': 66,\n",
       " 'eins': 66,\n",
       " 'entschuldigung': 66,\n",
       " 'er': 66,\n",
       " 'erregend': 66,\n",
       " 'es': 66,\n",
       " 'everywhere': 10,\n",
       " 'excitatory': 41,\n",
       " 'extrapolating': 31,\n",
       " 'falsch': 66,\n",
       " 'farbenblind': 66,\n",
       " 'gelahmt': 66,\n",
       " 'geschlagen': 66,\n",
       " 'haben': 66,\n",
       " 'hausangestellte': 66,\n",
       " 'he': 39,\n",
       " 'here': 52,\n",
       " 'hier': 66,\n",
       " 'i': 11,\n",
       " 'ich': 66,\n",
       " 'in': 35,\n",
       " 'infectious': 32,\n",
       " 'interesse': 66,\n",
       " 'interessiert': 66,\n",
       " 'interested': 19,\n",
       " 'is': 27,\n",
       " 'ist': 66,\n",
       " 'jetzt': 66,\n",
       " 'joking': 42,\n",
       " 'kidding': 51,\n",
       " 'kritisch': 66,\n",
       " 'lecker': 66,\n",
       " 'leite': 66,\n",
       " 'liegen': 66,\n",
       " 'live': 33,\n",
       " 'm': 25,\n",
       " 'mammals': 47,\n",
       " 'moving': 30,\n",
       " 'nackt': 66,\n",
       " 'neugierig': 66,\n",
       " 'now': 28,\n",
       " 'nude': 8,\n",
       " 'obsolete': 43,\n",
       " 'one': 44,\n",
       " 'oppressed': 14,\n",
       " 'over': 34,\n",
       " 'paralyzed': 49,\n",
       " 'perfect': 26,\n",
       " 'perfekt': 66,\n",
       " 'powerful': 45,\n",
       " 'productive': 13,\n",
       " 'produktiv': 66,\n",
       " 'psychiaterinnen': 66,\n",
       " 'psychiatrists': 53,\n",
       " 're': 9,\n",
       " 'reconstructive': 54,\n",
       " 'rekonstruktiv': 66,\n",
       " 'responsible': 58,\n",
       " 's': 7,\n",
       " 'saugetiere': 66,\n",
       " 'schaut': 66,\n",
       " 'scherze': 66,\n",
       " 'schluchzt': 66,\n",
       " 'schockiert': 66,\n",
       " 'schokolade': 66,\n",
       " 'sells': 61,\n",
       " 'she': 6,\n",
       " 'shocked': 21,\n",
       " 'sie': 66,\n",
       " 'sind': 66,\n",
       " 'singen': 66,\n",
       " 'singing': 60,\n",
       " 'smart': 50,\n",
       " 'sniveling': 40,\n",
       " 'sorry': 57,\n",
       " 'staatenlos': 66,\n",
       " 'stark': 66,\n",
       " 'stateless': 59,\n",
       " 'they': 2,\n",
       " 'three': 18,\n",
       " 'trage': 66,\n",
       " 'uberall': 66,\n",
       " 'und': 66,\n",
       " 'unrecht': 66,\n",
       " 'uns': 66,\n",
       " 'unterdruckt': 66,\n",
       " 'veraltet': 66,\n",
       " 'verantwortung': 66,\n",
       " 'verkauft': 66,\n",
       " 'vorbei': 66,\n",
       " 'voreingenommen': 66,\n",
       " 'watching': 37,\n",
       " 'we': 29,\n",
       " 'werden': 66,\n",
       " 'wir': 66,\n",
       " 'worried': 23,\n",
       " 'wrong': 24,\n",
       " 'wurde': 66,\n",
       " 'you': 16,\n",
       " 'zermalmt': 66,\n",
       " 'zu': 66}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 35,\n",
       " '.': 5,\n",
       " 'ab': 32,\n",
       " 'ansteckend': 33,\n",
       " 'auftragsarbeiten': 70,\n",
       " 'besorgt': 21,\n",
       " 'bewegen': 29,\n",
       " 'bin': 11,\n",
       " 'christen': 18,\n",
       " 'clever': 56,\n",
       " 'delikatessen': 43,\n",
       " 'die': 51,\n",
       " 'drei': 16,\n",
       " 'dynamisch': 61,\n",
       " 'eins': 49,\n",
       " 'entschuldigung': 62,\n",
       " 'er': 44,\n",
       " 'erregend': 46,\n",
       " 'es': 69,\n",
       " 'falsch': 23,\n",
       " 'farbenblind': 71,\n",
       " 'gelahmt': 55,\n",
       " 'geschlagen': 39,\n",
       " 'haben': 25,\n",
       " 'hausangestellte': 54,\n",
       " 'hier': 57,\n",
       " 'ich': 10,\n",
       " 'in': 37,\n",
       " 'interesse': 42,\n",
       " 'interessiert': 17,\n",
       " 'ist': 6,\n",
       " 'jetzt': 27,\n",
       " 'kritisch': 15,\n",
       " 'lecker': 52,\n",
       " 'leite': 31,\n",
       " 'liegen': 22,\n",
       " 'live': 34,\n",
       " 'nackt': 7,\n",
       " 'neugierig': 20,\n",
       " 'perfekt': 24,\n",
       " 'produktiv': 12,\n",
       " 'psychiaterinnen': 58,\n",
       " 'rekonstruktiv': 59,\n",
       " 'saugetiere': 53,\n",
       " 'schaut': 40,\n",
       " 'scherze': 47,\n",
       " 'schluchzt': 45,\n",
       " 'schockiert': 19,\n",
       " 'schokolade': 68,\n",
       " 'sie': 2,\n",
       " 'sind': 8,\n",
       " 'singen': 66,\n",
       " 'staatenlos': 65,\n",
       " 'stark': 50,\n",
       " 'trage': 63,\n",
       " 'uberall': 9,\n",
       " 'und': 14,\n",
       " 'unrecht': 26,\n",
       " 'uns': 30,\n",
       " 'unterdruckt': 13,\n",
       " 'veraltet': 48,\n",
       " 'verantwortung': 64,\n",
       " 'verkauft': 67,\n",
       " 'vorbei': 36,\n",
       " 'voreingenommen': 60,\n",
       " 'werden': 3,\n",
       " 'wir': 28,\n",
       " 'wurde': 38,\n",
       " 'zermalmt': 4,\n",
       " 'zu': 41}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> sie sind neugierig .\n",
      "= they are curious .\n",
      "< they are curious . <EOS>\n",
      "\n",
      "> wir sind eins .\n",
      "= we are one .\n",
      "< we are one . <EOS>\n",
      "\n",
      "> ich trage verantwortung .\n",
      "= i m responsible .\n",
      "< i m responsible . <EOS>\n",
      "\n",
      "> sie sind staatenlos .\n",
      "= they are stateless .\n",
      "< they are stateless . <EOS>\n",
      "\n",
      "> ich bin interessiert .\n",
      "= i m interested .\n",
      "< i m biased . <EOS>\n",
      "\n",
      "> ich bin stark .\n",
      "= i m powerful .\n",
      "< i m colorblind . <EOS>\n",
      "\n",
      "> sie sind drei .\n",
      "= they re three .\n",
      "< they re three . <EOS>\n",
      "\n",
      "> sie sind live !\n",
      "= you re live .\n",
      "< you re wrong . <EOS>\n",
      "\n",
      "> es sind auftragsarbeiten .\n",
      "= they are commissioned .\n",
      "< they are commissioned . <EOS>\n",
      "\n",
      "> sie sind und .\n",
      "= they re and .\n",
      "< they re dynamic . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
