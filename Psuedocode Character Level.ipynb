{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharLevel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, Tx, m, w, N):\n",
    "        super(CharLevel, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #what's w?\n",
    "        #what's N? Where do we define all the scalar values needed for this?\n",
    "        #emb_dim is called d_c in the paper\n",
    "        \n",
    "        \n",
    "        \"\"\"I think I'm most confused about the convolution part. The paper doesn't use the same\n",
    "        vocabulary as the documentaton for torch's built-in convolution layers, and I'm not sure sure\n",
    "        if the built-in convolution options are enough to do it the same way as the paper\"\"\"\n",
    "        #this seems helpful:\n",
    "        #https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "        \n",
    "        #do we pad here or in the convolution layer? Both have a padding option.\n",
    "        \n",
    "        # I think filtering isn't a separate layer because we can filter using the parameters.\n",
    "        # I think the kernel is the \"filter\" and \"stride\" controls how the filter convolves around the input volume.\n",
    "        # Not sure about \"groups\" which is the \"Number of blocked connections from input channels to output channels.\"\n",
    "        \n",
    "        #are there multiple conv layers for different ngram lengths so we can perform this on all ngrams \n",
    "        #up to 8 characters long or are we feeding into the model every length of ngram at once?\n",
    "        #so the preprocessing steps would need to preprocess the data with ngrams from 1-8?\n",
    "        \n",
    "        #not sure if I have Tx, N, w all in the right place for the parameter values\n",
    "        \n",
    "        self.convolve = nn.Conv1d(Tx, N, w, padding=w-1, bias=False)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #I think both the kernel and the stride are size 5 so that it's non-overlapping\n",
    "        self.maxpool = nn.MaxPool1d(kernel = 5, stride=5)\n",
    "        \n",
    "        #this is all part of the \"highway\"\n",
    "        #got the inspiration for this from\n",
    "        #https://github.com/c0nn3r/pytorch_highway_networks/blob/master/layers/highway.py\n",
    "        self.activation_function = nn.functional.relu\n",
    "        self.gate_activation = nn.functional.softmax #I think the paper doesn't say\n",
    "        #whether this is softmax or something else - could be sigmoid?\n",
    "\n",
    "        self.normal_layer = nn.Linear(input_size, input_size)\n",
    "\n",
    "        self.gate_layer = nn.Linear(input_size, input_size)\n",
    "        self.gate_layer.bias.data.fill_(gate_bias)\n",
    "        \n",
    "        #done with the highway\n",
    "        \n",
    "        self.GRU = nn.GRU(bidirectional=True)\n",
    "        self.init_weights()\n",
    "        \n",
    "                \n",
    "    def forward(self, x, hidden, c):\n",
    "        \"\"\"What goes in the \"forward\" and what goes in the \"step\"?\n",
    "        \"Step\" has to be called in the \"forward\" so I think it doesn't matter, it's just to keep things organized\"\"\"\n",
    "        #Embedding\n",
    "        x_emb = self.encoder(x)\n",
    "        x_conv = self.convolve(x_emb)\n",
    "        x_relu = self.relu(x_conv)\n",
    "        x_MaxPool = self.maxpool(x_relu)\n",
    "        ###this is all for the \"highway\"\n",
    "        normal_layer_result = self.activation_function(self.normal_layer(x_MaxPool))\n",
    "        gate_layer_result = self.gate_activation(self.gate_layer(x_MaxPool))\n",
    "\n",
    "        multiplyed_gate_and_normal = torch.mul(normal_layer_result, gate_layer_result)\n",
    "        multiplyed_gate_and_input = torch.mul((1 - gate_layer_result), x_MaxPool)\n",
    "\n",
    "        x_highway = torch.add(multiplyed_gate_and_normal,\n",
    "                         multiplyed_gate_and_input)\n",
    "    \n",
    "        return self.GRU(x_highway)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Blabla not sure what goes here\"\"\"\n",
    "                \n",
    "    def init_hidden(self, bsz):\n",
    "        \"\"\"Blabla I don't think we need this\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
