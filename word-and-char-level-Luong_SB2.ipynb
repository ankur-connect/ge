{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "You will need [PyTorch](http://pytorch.org/) to build and train the models, and [matplotlib](https://matplotlib.org/) for plotting training and visualizing attention outputs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from collections import Counter\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim.lr_scheduler as lr\n",
    "from random import shuffle\n",
    "import math\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open(\"/Users/carolineroper/Documents/School/Natural Language Processing/Char-NMT/data/train.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "with open(\"/Users/bilals01/Documents/NLP/project/Char-NMT/data/train.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    DE_seq = []\n",
    "    EN_seq = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        DE_seq.append(line[0])\n",
    "        EN_seq.append(line[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing words\n",
    "\n",
    "We'll need a unique index per word to use as the inputs and targets of the networks later. To keep track of all this we will use a helper class called `Lang` which has word &rarr; index (`word2index`) and index &rarr; word (`index2word`) dictionaries, as well as a count of each word `word2count` to use to later replace rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and decoding files\n",
    "\n",
    "The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = s.replace(\" <EOS>\", \"\")\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) #separates punctuation from the word\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) #strips anything that isn't a character of punctuation\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(seq1, seq2):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    seq1 = [normalizeString(s) for s in seq1]\n",
    "    seq2 = [normalizeString(s) for s in seq2]\n",
    "    \n",
    "    pairs = list(map(list, zip(seq1, seq2)))\n",
    "\n",
    "    return pairs #, vocab1, vocab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    }
   ],
   "source": [
    "pairs = readLangs(DE_seq, EN_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering sentences\n",
    "\n",
    "Since there are a *lot* of example sentences and we want to train something quickly, we'll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes punctuation) and we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. (accounting for apostrophes being removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#will ultimately remove this and set MAX_LENGTH to either 50 like the paper or the longest sentence in corpus\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH  \\\n",
    "        and p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "pairs = filterPairs(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_vocabulary(seq, vocab_size):\n",
    "    counter = Counter()\n",
    "    for sentence in seq:\n",
    "        counter.update(sentence.split())\n",
    "    vocabulary = [count[0] for count in counter.most_common(vocab_size)]\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "\n",
    "def find_all_vocabulary(seq):\n",
    "    counter = Counter()\n",
    "    for sentence in seq:\n",
    "        counter.update(sentence.split())\n",
    "    vocabulary = [count[0] for count in counter.most_common(100000000000000)]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "input_vocab = find_vocabulary([pair[0] for pair in pairs], vocab_size)\n",
    "output_vocab = find_vocabulary([pair[1] for pair in pairs], vocab_size)\n",
    "\n",
    "input_a_w = find_all_vocabulary([pair[0] for pair in pairs])\n",
    "output_a_w = find_all_vocabulary([pair[1] for pair in pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNK_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "PAD_token = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, allwords, vocab):\n",
    "        self.name = name\n",
    "        self.vocab = vocab\n",
    "        self.allwords = allwords\n",
    "\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word_vocab = dict(zip(range(4,len(vocab)+4), vocab))\n",
    "        self.index2word_allwords = dict(zip(range(4,len(allwords)+4), allwords))\n",
    "\n",
    "        self.index2word_vocab[UNK_token] = \"<UNK>\"\n",
    "        self.index2word_vocab[SOS_token] = \"<SOS>\"\n",
    "        self.index2word_vocab[EOS_token] = \"<EOS>\"\n",
    "        self.index2word_vocab[PAD_token] = \"<PAD>\"\n",
    "        \n",
    "        \n",
    "        self.word2index_vocab = {v: k for k, v in self.index2word_vocab.items()} \n",
    "        self.word2index_allwords = {v: k for k, v in self.index2word_allwords.items()} \n",
    "\n",
    "        self.n_words = len(self.index2word_vocab)\n",
    "        self.n_words_all = len(self.index2word_allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_lang = Lang('ger',input_a_w, input_vocab)\n",
    "output_lang = Lang('eng', output_a_w,output_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "* Read text file and split into lines, split lines into pairs\n",
    "* Normalize text, filter by length and content\n",
    "* Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence, seq_len):\n",
    "    \n",
    "    type(sentence)\n",
    "    \n",
    "    #returns 0 if not found in word2index\n",
    "    indexes0 = [Counter(lang.word2index_vocab)[word] for word in sentence.split(' ')]\n",
    "    indexes1 = [Counter(lang.word2index_allwords)[word] for word in sentence.split(' ')]\n",
    "\n",
    "    ind1=[]#regular indices\n",
    "    ind2=[]#indices of the exotic words\n",
    "\n",
    "    for ind in range(0, len(indexes0)): \n",
    "        ind1.append(indexes0[ind])\n",
    "        if (ind1[ind]==0):\n",
    "            ind2.append(indexes1[ind])\n",
    "        else: \n",
    "            ind2.append(-1) #fill the non exotic with -1\n",
    "\n",
    "        \n",
    "    #adds EOS token at EOS\n",
    "    ind1.extend([EOS_token])\n",
    "    ind2.extend([EOS_token]) #do we need that, not sure if we care if the char matrix has EOS and SOS\n",
    "\n",
    "    #trims to the seq len\n",
    "    ind1 = ind1[0:(min(seq_len, len(ind1)))]\n",
    "    ind2 = ind2[0:(min(seq_len, len(ind2)))] \n",
    "\n",
    "\n",
    "    #pads if needed\n",
    "    ind1.extend([3] * (seq_len - len(ind1)))\n",
    "    ind2.extend([3] * (seq_len - len(ind2)))\n",
    "\n",
    "    return (ind1, ind2)\n",
    "\n",
    "def variableFromSentence(lang, sentence, seq_len):\n",
    "    (ind1, ind2) = indexesFromSentence(lang, sentence, seq_len)\n",
    "    result1 = Variable(torch.LongTensor(ind1).view(-1, 1))\n",
    "    result2 = Variable(torch.LongTensor(ind2).view(-1, 1))\n",
    "\n",
    "    if USE_CUDA:\n",
    "        return (result1.cuda(), result2.cuda())\n",
    "    else:\n",
    "        return (result1, result2)\n",
    "\n",
    "def variablesFromPair(pair, seq_len):\n",
    "    (input_variable1, input_variable2)  = variableFromSentence(input_lang, pair[0], seq_len)\n",
    "    (target_variable1, target_variable2) = variableFromSentence(output_lang, pair[1], seq_len)\n",
    "    return (input_variable1, target_variable1, input_variable2, target_variable2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "char_vocab = list(string.printable)\n",
    "\n",
    "UNK_char = 0\n",
    "SOS_char = 1\n",
    "EOS_char = 2\n",
    "PAD_char = 3\n",
    "\n",
    "char_dict=dict(zip(range(4,len(char_vocab)+4), char_vocab)) \n",
    "char2index_counter={v: k for k, v in char_dict.items()}\n",
    "\n",
    "def indexesFromWord(word,seq_len):\n",
    "    indexes = [Counter(char2index_counter)[chars] for chars in list(word)]\n",
    "    indexes.extend([EOS_char])\n",
    "    indexes = indexes[0:(min(seq_len, len(indexes)))]\n",
    "    indexes.extend([3] * (seq_len - len(indexes)))\n",
    "    return indexes\n",
    "\n",
    "#print(indexesFromWord(\"dghfgf\",10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Variable containing:\n",
       "     77\n",
       "      6\n",
       "      5\n",
       "    179\n",
       "     62\n",
       "     12\n",
       "    561\n",
       "      4\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "      6\n",
       "     10\n",
       "    109\n",
       "     21\n",
       "    146\n",
       "    532\n",
       "     33\n",
       "      4\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1]), (Variable containing:\n",
       "     16\n",
       "      6\n",
       "    562\n",
       "      4\n",
       "      2\n",
       "      3\n",
       "      3\n",
       "      3\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "      6\n",
       "      5\n",
       "    533\n",
       "    534\n",
       "      4\n",
       "      2\n",
       "      3\n",
       "      3\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "      2\n",
       "      3\n",
       "      3\n",
       "      3\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "      2\n",
       "      3\n",
       "      3\n",
       "  [torch.LongTensor of size 8x1]), (Variable containing:\n",
       "      7\n",
       "      9\n",
       "    152\n",
       "     34\n",
       "     23\n",
       "    563\n",
       "    564\n",
       "      4\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "      7\n",
       "      9\n",
       "    177\n",
       "     43\n",
       "     15\n",
       "    303\n",
       "    535\n",
       "      4\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1]), (Variable containing:\n",
       "     13\n",
       "     10\n",
       "    565\n",
       "     13\n",
       "     10\n",
       "     23\n",
       "    566\n",
       "    567\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     17\n",
       "     13\n",
       "    178\n",
       "     17\n",
       "     13\n",
       "     15\n",
       "     91\n",
       "    536\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1]), (Variable containing:\n",
       "      7\n",
       "      9\n",
       "    109\n",
       "    315\n",
       "    180\n",
       "     31\n",
       "     54\n",
       "    568\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "      7\n",
       "     23\n",
       "    537\n",
       "     62\n",
       "     59\n",
       "    147\n",
       "     36\n",
       "     63\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1]), (Variable containing:\n",
       "    122\n",
       "     48\n",
       "    110\n",
       "      4\n",
       "     16\n",
       "     10\n",
       "     44\n",
       "      4\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "      7\n",
       "      9\n",
       "     72\n",
       "      4\n",
       "     30\n",
       "     13\n",
       "    538\n",
       "      4\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1]), (Variable containing:\n",
       "      8\n",
       "    569\n",
       "    153\n",
       "     38\n",
       "     78\n",
       "    570\n",
       "    571\n",
       "      4\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "      8\n",
       "      5\n",
       "     91\n",
       "     92\n",
       "     45\n",
       "     18\n",
       "    213\n",
       "    539\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1]), (Variable containing:\n",
       "      5\n",
       "     21\n",
       "    572\n",
       "      4\n",
       "      2\n",
       "      3\n",
       "      3\n",
       "      3\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "      6\n",
       "     10\n",
       "    540\n",
       "      4\n",
       "      2\n",
       "      3\n",
       "      3\n",
       "      3\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "      2\n",
       "      3\n",
       "      3\n",
       "      3\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "      2\n",
       "      3\n",
       "      3\n",
       "      3\n",
       "  [torch.LongTensor of size 8x1]), (Variable containing:\n",
       "      5\n",
       "    573\n",
       "    574\n",
       "     18\n",
       "    575\n",
       "      4\n",
       "      2\n",
       "      3\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "      6\n",
       "      5\n",
       "    541\n",
       "    542\n",
       "    543\n",
       "     31\n",
       "    214\n",
       "    544\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "      2\n",
       "      3\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1]), (Variable containing:\n",
       "      8\n",
       "    576\n",
       "     33\n",
       "     38\n",
       "     29\n",
       "    577\n",
       "     97\n",
       "    578\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "      8\n",
       "      5\n",
       "    304\n",
       "     33\n",
       "     34\n",
       "    545\n",
       "    546\n",
       "    547\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1], Variable containing:\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "     -1\n",
       "  [torch.LongTensor of size 8x1])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_pairs = pairs[0:10]\n",
    "batch_input = [pair[0] for pair in pairs]\n",
    "batch_target = [pair[1] for pair in pairs]\n",
    "batch_vars = [variablesFromPair(pair, seq_len=8) for pair in batch_pairs]\n",
    "    \n",
    "\n",
    "batch_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "<img src=\"images/encoder-network.png\" style=\"float: right\" />\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterEncoderRNN(nn.Module):\n",
    "    #different batch size?\n",
    "    #self.batch_size\n",
    "    #different self.input_size?\n",
    "    \n",
    "    #takes a word or batch of words\n",
    "    #loops through each of the characters or batch of characters at one index position\n",
    "    #f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, batch_size, uniform_init=False, dropout_p=0.2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size) #the input size is the number of words\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, dropout = dropout_p)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        #used in paper but isn't working well on the subsample of data\n",
    "        if uniform_init == True:\n",
    "            self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        seq_len = len(x)\n",
    "        \n",
    "        embedded = self.embedding(x).view(seq_len, self.batch_size, -1) #this will be 1x128x500\n",
    "        output = self.dropout(embedded)\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    #also return indexes_where_unknown\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.lstm.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.weight_hh_l0.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.bias_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.bias_hh_l0.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        #the paper says to use \"uniform initialization of parameters in [−0.1,0.1]: does that include these?\"\n",
    "        h0 = Variable(torch.zeros(1, 1, self.hidden_size)) #I think maybe one of the 1's should be the batch_size?\n",
    "        c0 = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if USE_CUDA:\n",
    "            return h0.cuda(), c0.cuda()\n",
    "        else:\n",
    "            return h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size, batch_size, method = 'general'):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        attn_energies = Variable(torch.zeros(self.batch_size, seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[:,i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        # I confirmed that this computes row-wise, which is what we need\n",
    "        after_softmax = F.softmax(attn_energies)\n",
    "        return after_softmax.unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = torch.diag(torch.matmul(hidden, torch.transpose(encoder_output,0,1)))\n",
    "            #print (\"energy\", energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = torch.diag(torch.matmul(hidden, torch.transpose(energy,0,1)))\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a decoder that plugs this Attn module in after the RNN to calculate attention weights, and apply those weights to the encoder outputs to get a context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers, batch_size, uniform_init = False, dropout_p=0.2):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.attn = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(hidden_size*2, hidden_size, dropout = dropout_p)\n",
    "        self.out = nn.Linear(hidden_size*2, output_size)\n",
    "        \n",
    "        if uniform_init == True:\n",
    "            self.init_weights()\n",
    "    \n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        \n",
    "        hidden = last_hidden\n",
    "        word_embedded = self.embedding(word_input).view(1, self.batch_size, -1) # S=1 x B x N\n",
    "        output = word_embedded\n",
    "\n",
    "        #Combine embedded input word and last context, run through LSTM\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            output = torch.cat((output, last_context.unsqueeze(0)), 2)\n",
    "            output, hidden = self.lstm(output, hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(word_embedded.squeeze(0), encoder_outputs)\n",
    "        context = torch.bmm(attn_weights.transpose(0, 1), encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Final output layer (next word prediction) using the hidden state and context vector\n",
    "        output = output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1) # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        #here is where we'd want to do any type of character-level thing\n",
    "        #ignore the \"separate path\" for now\n",
    "        #\n",
    "        \n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.lstm.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.weight_hh_l0.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.bias_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.bias_hh_l0.data.uniform_(-initrange, initrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderCharLevelRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers, batch_size, uniform_init = False, dropout_p=0.2):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.attn = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(hidden_size*2, hidden_size, dropout = dropout_p)\n",
    "        self.out = nn.Linear(hidden_size*2, output_size)\n",
    "        \n",
    "        if uniform_init == True:\n",
    "            self.init_weights()\n",
    "    \n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        \n",
    "        hidden = last_hidden\n",
    "        word_embedded = self.embedding(word_input).view(1, self.batch_size, -1) # S=1 x B x N\n",
    "        output = word_embedded\n",
    "\n",
    "        #Combine embedded input word and last context, run through LSTM\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            output = torch.cat((output, last_context.unsqueeze(0)), 2)\n",
    "            output, hidden = self.lstm(output, hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(word_embedded.squeeze(0), encoder_outputs)\n",
    "        context = torch.bmm(attn_weights.transpose(0, 1), encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Final output layer (next word prediction) using the hidden state and context vector\n",
    "        output = output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1) # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.lstm.weight_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.weight_hh_l0.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.bias_ih_l0.data.uniform_(-initrange, initrange)\n",
    "        self.lstm.bias_hh_l0.data.uniform_(-initrange, initrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Defining a training iteration\n",
    "\n",
    "To train we first run the input sentence through the encoder word by word, and keep track of every output and the latest hidden state. Next the decoder is given the last hidden state of the decoder as its first hidden state, and the `<SOS>` token as its first input. From there we iterate to predict a next token from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f8512eec26d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;31m#will be 128 in final version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mN_LAYERS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mSEQ_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m#will be 1 in final version, descends by scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_size = 500 #will be 1024 in final version\n",
    "BATCH_SIZE = 64 #will be 128 in final version\n",
    "N_LAYERS = 4\n",
    "SEQ_LENGTH = MAX_LENGTH\n",
    "learning_rate = 1 #will be 1 in final version, descends by scheduler\n",
    "\n",
    "attn_model = Attn(hidden_size, BATCH_SIZE)\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, \\\n",
    "                     n_layers=N_LAYERS, batch_size = BATCH_SIZE)\n",
    "decoder = AttnDecoderRNN(attn_model, hidden_size, output_lang.n_words,\\\n",
    "                         n_layers = N_LAYERS, batch_size=BATCH_SIZE) \n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "# commenting out for now because the paper's parameters didn't work well when I tried them)\n",
    "\n",
    "#encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) #should be optim.SGD according to the paper\n",
    "#decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "#scheduler1 = lr.LambdaLR(encoder_optimizer, lr_lambda=custom_schedule)\n",
    "#scheduler2 = lr.LambdaLR(decoder_optimizer, lr_lambda=custom_schedule)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "###these parameters are nothing like the paper but seemed to work best, at least with less data\n",
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-79564a0f0d98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Zero gradients of both optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# Added onto for each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Zero gradients of both optimizers\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "loss = 0 # Added onto for each word\n",
    "\n",
    "# Get size of input and target sentences\n",
    "input_length = input_variable.size()[0]\n",
    "target_length = target_variable.size()[0]\n",
    "\n",
    "# Run words through encoder\n",
    "encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "########## Need the final \"encoder outputs\" to incorporate both the wl & cl encodings?\n",
    "encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "# Prepare input and output variables\n",
    "# One SOS token for each sentence, so length is batch size\n",
    "decoder_input = Variable(torch.LongTensor([[SOS_token]*batch_size]))\n",
    "decoder_context = Variable(torch.zeros(batch_size, decoder.hidden_size))\n",
    "decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "if USE_CUDA:\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "for di in range(target_length):\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "\n",
    "    loss += criterion(decoder_output, target_variable[di])\n",
    "\n",
    "    # Get most likely word index (highest value) from output\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "\n",
    "    ni = topi.squeeze(1)\n",
    "\n",
    "    decoder_input = Variable(ni) # Chosen word is next input\n",
    "    if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "        \n",
    "    #Find the entries where ni = 0 - these are the <unks>\n",
    "    #When it produces an <>\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "encoder_optimizer.step()\n",
    "decoder_optimizer.step()\n",
    "\n",
    "return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#maybe the data loader needs a \"sentence index\" that is fixed (not dependent on batch)\n",
    "#so you receive a bach and a vector of indeces that corresponds to the index in the pair itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip = 5.0\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer,\\\n",
    "          criterion, batch_size, max_length):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        #input a column of word indexes, some high (exotic), some low (non-exotic)\n",
    "        #input_var_for_word_level = input with high indexes replaced with 0's\n",
    "        #run the input_var_for_word_level through the encoder\n",
    "        encoder_output, encoder_hidden, indexes_where_unknown = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0] #will probably need to change these subscripts\n",
    "        #input_var_for_char_level = rare words represented at the character level\n",
    "        #index_to_reunite with_word_level\n",
    "        #create char-level matrix\n",
    "        #use some time of special token as a placeholder to represent where the\n",
    "        #input word is within the vocabulary\n",
    "        char_encoder_output, char_encoder_hidden = character_encoder_goes_here\n",
    "        \n",
    "        \n",
    "        #replace the word-level with char-level where needed\n",
    "        encoder_outputs[ei][indexes_where_unknown] = char_encoder_output\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    # One SOS token for each sentence, so length is batch size\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]*batch_size]))\n",
    "    decoder_context = Variable(torch.zeros(batch_size, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "        \n",
    "    for di in range(target_length):\n",
    "        #possible I'll need to move this inside the AttnDecoderRNN so that\n",
    "        #the torch autogradient thing works\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "        \n",
    "        # Get most likely word index (highest value) from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        \n",
    "        ni = topi.squeeze(1)\n",
    "        \n",
    "        \n",
    "\n",
    "        decoder_input = Variable(ni) # Chosen word is next input\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "        #once the decoder output is produced - can see if there are <UNKs> = 0\n",
    "        #wherever there's a 0, seek to replace the 0 with a string of characters similar to the \n",
    "        #target[di]\n",
    "        #so find the indexes where the decoded outputs = 0\n",
    "        #grab the attributes that the decoder needs in order to function at those indexes\n",
    "        #(might not actually need all of these depending on the architecture of the decoder):\n",
    "            #to avoid dynamic batch size, put placeholder special character where\n",
    "            #word is in vocabulary\n",
    "            #decoder_output[indexes resulted in 0], \n",
    "            #decoder_context[indexes resulted in 0],\n",
    "            #decoder_hidden[indexes resulted in 0], \n",
    "            #decoder_attention[indexes resulted in 0]\n",
    "        #pass to the character-level decoder, find the most likely character at each position in the sequence\n",
    "        #replace the decoder output at those indexes with the character-level sequence\n",
    "            #evaluate the character-level loss between target_variable[di] (represented via character indexes) &\n",
    "            #the output of the decoder\n",
    "            #I think it'll have its own cl_loss.backward() to perfect it\n",
    "        #the actual loss is then the weighed average of these two things - the reason the losses are connected\n",
    "        #is because the word-level piece is also giving output to the character level piece\n",
    "        #so you should think of the character-level loss as a component of the word-level loss\n",
    "         \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally helper functions to print time elapsed and estimated time remaining, given the current time and progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running training\n",
    "\n",
    "With everything in place we can actually initialize a network and start training.\n",
    "\n",
    "To start, we initialize models, optimizers, and a loss function (criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_schedule(epoch):\n",
    "    if (epoch < 4):\n",
    "        return (1)\n",
    "    elif epoch == 4:\n",
    "        return (.5)\n",
    "    elif epoch >= 4:\n",
    "        return (.5**(1+(epoch - 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 500 #will be 1024 in final version\n",
    "BATCH_SIZE = 64 #will be 128 in final version\n",
    "N_LAYERS = 4\n",
    "SEQ_LENGTH = MAX_LENGTH\n",
    "learning_rate = 1 #will be 1 in final version, descends by scheduler\n",
    "\n",
    "attn_model = Attn(hidden_size, BATCH_SIZE)\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, \\\n",
    "                     n_layers=N_LAYERS, batch_size = BATCH_SIZE)\n",
    "decoder = AttnDecoderRNN(attn_model, hidden_size, output_lang.n_words,\\\n",
    "                         n_layers = N_LAYERS, batch_size=BATCH_SIZE) \n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "# commenting out for now because the paper's parameters didn't work well when I tried them)\n",
    "\n",
    "#encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) #should be optim.SGD according to the paper\n",
    "#decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "#scheduler1 = lr.LambdaLR(encoder_optimizer, lr_lambda=custom_schedule)\n",
    "#scheduler2 = lr.LambdaLR(decoder_optimizer, lr_lambda=custom_schedule)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "###these parameters are nothing like the paper but seemed to work best, at least with less data\n",
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set up variables for plotting and tracking progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, seq_length):\n",
    "        \"\"\"\n",
    "        @param data_list: list of IMDBDatum\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        return indexesFromSentence(input_lang, self.data_list[key][0], seq_len = self.seq_length), \\\n",
    "    indexesFromSentence(output_lang, self.data_list[key][1], seq_len = self.seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(pairs, SEQ_LENGTH)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "print_loss_num = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 7s (- 1m 7s) 2.8404785633087157\n",
      "2m 14s (- 0m 0s) 1.358345284461975\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,n_epochs+1):\n",
    "    for i, (input_tensor, target_tensor, char_level_input, char_level_target(?)) in enumerate(train_loader):\n",
    "        if input_tensor.size()[0]==BATCH_SIZE:\n",
    "            #this is needed because the loader puts the remainder datum in a too-small batch\n",
    "            input_tensor = input_tensor.squeeze(1).transpose(0,1)\n",
    "            target_tensor = target_tensor.squeeze(1).transpose(0,1)\n",
    "            input_variable = Variable(input_tensor)\n",
    "            target_variable = Variable(target_tensor)\n",
    "            if USE_CUDA:\n",
    "                input_variable = input_variable.cuda()\n",
    "                target_variable = target_variable.cuda()\n",
    "\n",
    "            loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer,\\\n",
    "                         criterion, BATCH_SIZE, SEQ_LENGTH)\n",
    "\n",
    "            # Keep track of loss\n",
    "            print_loss_total += loss\n",
    "            print_loss_num += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    print_loss_avg = print_loss_total / print_loss_num\n",
    "    print_loss_total = 0\n",
    "    print_loss_num = 0\n",
    "    print (time_since(start, epoch/n_epochs), print_loss_avg)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the network\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets. Instead we always feed the decoder's predictions back to itself. Every time it predicts a word, we add it to the output string. If it predicts the EOS token we stop there. We also store the decoder's attention outputs for each step to display later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(pairs, seq_length = SEQ_LENGTH, batch_size = BATCH_SIZE, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # get a random batch\n",
    "    shuffle(pairs)\n",
    "    batch_pairs = pairs[0:batch_size]\n",
    "    batch_input = [pair[0] for pair in pairs]\n",
    "    batch_target = [pair[1] for pair in pairs]\n",
    "    batch_vars = [variablesFromPair(pair, seq_len=SEQ_LENGTH) for pair in batch_pairs]\n",
    "    \n",
    "    # stack that batch\n",
    "    inputs = [variable[0].data for variable in batch_vars]\n",
    "    input_variable = Variable(torch.stack(inputs, 1).squeeze(), volatile=False)\n",
    "    targets = [variable[1].data for variable in batch_vars]\n",
    "    target_variable = Variable(torch.stack(targets, 1).squeeze(), volatile=False)\n",
    "    \n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    # One SOS token for each stence, so length is batch size\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]*batch_size]))\n",
    "    decoder_context = Variable(torch.zeros(batch_size, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Run through decoder\n",
    "    \n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi.squeeze(1)\n",
    "        \n",
    "        batch_decoded_words = []\n",
    "        for word_index in ni.numpy():\n",
    "            batch_decoded_words.append(output_lang.index2word[word_index])\n",
    "        \n",
    "        decoded_words.append(batch_decoded_words)\n",
    "            \n",
    "        decoder_input = Variable(ni) # Chosen word is next input\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "            \n",
    "    decoded_words_matrix = np.array(decoded_words).reshape((SEQ_LENGTH, BATCH_SIZE))\n",
    "        \n",
    "    decoded_sentences = []\n",
    "\n",
    "    for column in range(decoded_words_matrix.shape[1]):\n",
    "        decoded_sentence = ' '.join(list(decoded_words_matrix[:, column]))\n",
    "        decoded_sentences.append(decoded_sentence)\n",
    "    \n",
    "    return batch_input, batch_target, decoded_sentences, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example_input, example_target, decoded_sentences, decoder_attentions = evaluate(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: sie sind durch und durch naturlich .\n",
      "target: they re completely natural .\n",
      "machine translation: they re very imaginative . <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "...\n",
      "input: es tut mir so leid . nein nein .\n",
      "target: i m so sorry . no no .\n",
      "machine translation: i m so sorry to say no . <EOS> <PAD>\n",
      "...\n",
      "input: sie werden von den anderen menschen beobachtet .\n",
      "target: they are observed by the other people \n",
      "machine translation: they are observed buried tuxedos other people . <EOS> <PAD>\n",
      "...\n",
      "input: wir sind noch nicht fertig mit dem video .\n",
      "target: we re not finished with the video yet .\n",
      "machine translation: we re not finished with with video yet . <EOS>\n",
      "...\n",
      "input: im einschatzen von wahrscheinlichkeiten sind wir nicht gut .\n",
      "target: we are not good at reasoning with uncertainty .\n",
      "machine translation: we are seeing a at reasoning with uncertainty . <EOS>\n",
      "...\n",
      "input: sie stellen tatsachlich urin her .\n",
      "target: they are actually making urine .\n",
      "machine translation: they are doing making urine . <EOS> <PAD> <PAD> <PAD>\n",
      "...\n",
      "input: wir werden das gleich erlautern .\n",
      "target: we re going to explain that right now .\n",
      "machine translation: we re going to stay with it forever . <EOS>\n",
      "...\n",
      "input: sie sind geradezu olympiasieger im mentalen training .\n",
      "target: they are real olympic champions of mind training .\n",
      "machine translation: they are the a champions might mind . <EOS> <PAD>\n",
      "...\n",
      "input: ich bin dein verbleibendes gedachtnis .\n",
      "target: i am your remaining memory .\n",
      "machine translation: i am an remaining memory . <EOS> <PAD> <PAD> <PAD>\n",
      "...\n",
      "input:  ihr seid doch amerika ! \n",
      "target: you re america . \n",
      "machine translation: you re america . <UNK> <EOS> <PAD> <PAD> <PAD> <PAD>\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print (\"input:\", example_input[i])\n",
    "    print (\"target:\", example_target[i])\n",
    "    print (\"translation:\", decoded_sentences[i])\n",
    "    print (\"...\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
